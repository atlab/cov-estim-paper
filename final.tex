% Based on Template for PLoS
% Version 2.0 July 2014
%
%
% -- FIGURES AND TABLES
%
% DO NOT INCLUDE GRAPHICS IN YOUR MANUSCRIPT
% - Figures should be uploaded separately from your manuscript file. 
% - Figures generated using LaTeX should be extracted and removed from the PDF before submission. 
% - Figures containing multiple panels/subfigures must be combined into one image file before submission.
% See http://www.plosone.org/static/figureGuidelines for PLOS figure guidelines.
%
% Tables should be cell-based and may not contain:
% - tabs/spacing/line breaks within cells to alter layout
% - vertically-merged cells (no tabular environments within tabular environments, do not use \multirow)
% - colors, shading, or graphic objects
% See http://www.plosone.org/static/figureGuidelines#tables for table guidelines.
%
% For sideways tables, use the {rotating} package and use \begin{sidewaystable} instead of \begin{table} in the appropriate section. PLOS guidelines do not accomodate sideways figures.
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% -- EQUATIONS, MATH SYMBOLS, SUBSCRIPTS, AND SUPERSCRIPTS
%
% IMPORTANT
% Below are a few tips to help format your equations and other special characters according to our specifications. For more tips to help reduce the possibility of formatting errors during conversion, please see our LaTeX guidelines at http://www.plosone.org/static/latexGuidelines
%
% Please be sure to include all portions of an equation in the math environment, and for any superscripts or subscripts also include the base number/text. For example, use $mathrm{mm}^2$ instead of mm$^2$ (do not use \textsuperscript command).
%
% DO NOT USE the \rm command to render mathmode characters in roman font, instead use $\mathrm{}$
% For bolding characters in mathmode, please use $\mathbf{}$ 
%
% Please add line breaks to long equations when possible in order to fit our 2-column layout. 
%
% For inline equations, please do not include punctuation within the math environment unless this is part of the equation.
%
% For spaces within the math environment please use the \; or \: commands, even within \text{} (do not use smaller spacing as this does not convert well).
%
%
% % % % % % % % % % % % % % % % % % % % % % % %



\documentclass[10pt]{article}

% amsmath package, useful for mathematical formulas
\usepackage{amsmath}
% amssymb package, useful for mathematical symbols
\usepackage{amssymb}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

\usepackage{hyperref}

% line numbers
\usepackage{lineno}

% ligatures disabled
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% rotating package for sideways tables
%\usepackage{rotating}

% If you wish to include algorithms, please use one of the packages below. Also, please see the algorithm section of our LaTeX guidelines (http://www.plosone.org/static/latexGuidelines) for important information about required formatting.
%\usepackage{algorithmic}
%\usepackage{algorithmicx}

% Use doublespacing - comment out for single spacing
%\usepackage{setspace} 
%\doublespacing


% Text layout
\topmargin 0.0cm
\oddsidemargin 0.5cm
\evensidemargin 0.5cm
\textwidth 16cm 
\textheight 21cm

% Bold the 'Figure #' in the caption and separate it with a period
% Captions will be left justified
\usepackage[labelfont=bf,labelsep=period,justification=raggedright]{caption}

% Use the PLoS provided BiBTeX style
\bibliographystyle{plos2009}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother


% Leave date blank
\date{}

\pagestyle{myheadings}

%% Include all macros below. Please limit the use of macros.
\DeclareMathOperator{\Tr}{tr}
\newcommand{\loss}[1]{\mathcal L(#1)}
\newcommand{\T}{{\sf T}}
\newcommand{\E}[2][]{\mathbb E_{#1}\left[ #2\right]}    % expected value
\DeclareMathOperator*{\argmin}{arg\,min}
%% END MACROS SECTION


\begin{document}


% Title must be 150 characters or less
\begin{flushleft}
{\Large
\textbf{Improved estimation and interpretation of correlations in neural circuits}
}
% Insert Author names, affiliations and corresponding author email.
\\
Dimitri Yatsenko$^{1}$,
Kre\v{s}imir Josi\'{c}$^{2}$,
Alexander S.~Ecker$^{1,3,4}$,
Emmanouil Froudarakis$^{1}$,
R.~James Cotton$^{1}$,
Andreas S.~Tolias$^{1,5,\ast}$
\\
\bf{1} Department of Neuroscience, Baylor College of Medicine, Houston, TX, USA
\\
\bf{2} Department of Mathematics and Department of Biology and Biochemistry, University of Houston, Houston, TX, USA
\\
\bf{3}  Werner Reichardt Center for Integrative Neuroscience and Institute for Theoretical Physics, University of T\"ubingen, Germany
\\
\bf{4} Bernstein Center for Computational Neuroscience, T\"ubingen, Germany
\\
\bf{5} Department of Computational and Applied Mathematics, Rice University, Houston, TX, USA

$\ast$ E-mail: atolias@cns.bcm.edu
\end{flushleft}

% Please keep the abstract between 250 and 300 words
\section*{Abstract}
Ambitious projects aim to record the activity of ever larger and denser neuronal populations \emph{in vivo}.  Correlations in neural activity measured in such recordings can reveal important aspects of  neural circuit organization.  However, estimating and interpreting large correlation matrices is statistically challenging.  Estimation can be improved by regularization, \emph{i.e.}\;by imposing a structure on the estimate.  The amount of improvement depends on how closely the assumed structure represents dependencies in the data. Therefore, the selection of the most efficient correlation matrix estimator for a given neural circuit must be determined empirically.  Importantly, the identity and structure of the most efficient estimator informs about the types of dominant dependencies governing the system.
We sought statistically efficient estimators of neural correlation matrices in recordings from large, dense groups of cortical neurons.  Using fast 3D random-access laser scanning microscopy of calcium signals, we recorded the activity of nearly every neuron in volumes 200 $\mu$m wide and 100 $\mu$m deep (150--350 cells) in mouse visual cortex.  We hypothesized that in these densely sampled recordings, the correlation matrix should be best modeled as the combination of a sparse graph of pairwise partial correlations representing interactions between the observed neuronal pairs and a low-rank component representing common fluctuations and external inputs.  Indeed, in cross-validation tests, the covariance matrix estimator with this structure consistently outperformed other regularized estimators. The sparse component of the estimate defined a graph of interactions. These interactions reflected the physical distances and orientation tuning properties of cells: The density of positive `excitatory' interactions decreased rapidly with geometric distances and with differences in orientation preference whereas negative `inhibitory' interactions were less selective.  Because of its superior performance, this `sparse + latent' estimator likely provides a more physiologically relevant representation of the functional connectivity in densely sampled recordings than the sample correlation matrix.

% Please keep the Author Summary between 150 and 200 words
% Use first person. PLOS ONE authors please skip this step. 
% Author Summary not valid for PLOS ONE submissions.   
\section*{Author Summary}
It is now possible to record the spiking activity of hundreds of neurons at the same time.  A meaningful statistical description of the collective activity of these neural populations -- their `functional connectivity' -- is a forefront challenge in neuroscience.  We addressed this problem by identifying statistically efficient estimators of correlation matrices of the spiking activity of neural populations.  Various underlying processes may reflect differently on the structure of the correlation matrix:  Correlations due to common network fluctuations or external inputs are well estimated by low-rank representations, whereas correlations arising from linear interactions between pairs of neurons are well approximated by their pairwise \emph{partial} correlations.  In our data obtained from fast 3D two-photon imaging of calcium signals of large and dense groups of neurons in mouse visual cortex, the best estimation performance was attained by decomposing the correlation matrix into a sparse network of partial correlations (`interactions') combined with a low-rank component. The inferred interactions were both positive (`excitatory') and negative (`inhibitory') and reflected the spatial organization and orientation preferences of the interacting cells.  We propose that  the most efficient among many estimators provides a more informative picture of the functional connectivity than previous analyses of neural correlations.


\section*{Introduction}
\emph{Functional connectivity} is a statistical description of observed \emph{multineuronal} activity patterns not reducible to the response properties of the individual cells. Functional connectivity reflects local synaptic connections, shared inputs from other regions, and endogenous network activity. Although functional connectivity is a phenomenological description without a strict mechanistic interpretation, it can be used to generate hypotheses about the anatomical architecture of the neural circuit and to test hypotheses about the processing of information at the population level.

Pearson correlations between the spiking activity of pairs of neurons are among the most familiar measures of functional connectivity \cite{Averbeck:2006, Zohary:1994, Kohn:2005, Bair:2001, Ecker:2010}.  In particular, \emph{noise correlations}, \emph{i.e.}\;the correlations of trial-to-trial response variability between pairs of neurons, have a profound impact on stimulus coding \cite{Zohary:1994, Abbott:1999, Sompolinsky:2001, Nirenberg:2003, Averbeck:2006, Josic:2009, Berens:2011, Ecker:2011}. In addition, noise correlations and correlations in spontaneous activity have been hypothesized to reflect aspects of synaptic connectivity \cite{Gerstein:1964}.  Interest in neural correlations has been sustained by a series of discoveries of their nontrivial relationships to various aspects of circuit organization such as the physical distances between the neurons \cite{Smith:2008, Denman:2013}, their synaptic connectivity \cite{Ko:2011},  stimulus response similarity \cite{Bair:2001, Arieli:1995, Chiu:2002, Kenet:2003, Kohn:2005, Cohen:2008, Cohen:2009, Ecker:2010, Rothschild:2010, Ko:2011, Smith:2013b}, cell types \cite{Hofer:2011}, cortical layer specificity \cite{Hansen:2012, Smith:2013}, progressive changes in development and in learning \cite{Golshani:2009, Gu:2011, Ko:2013}, changes due to sensory stimulation and global brain states \cite{Greenberg:2008, Goard:2009, Kohn:2009, Rothschild:2010, Ecker:2014, Renart:2010}.

Neural correlations do not come with ready or unambiguous mechanistic interpretations. They can arise from monosynaptic or polysynaptic interactions, common or correlated inputs, oscillations, top-down modulation, and background network fluctuations, and other mechanisms \cite{Perkel:1967, Moore:1970, Shadlen:1998, Salinas:2001, Ostojic:2009, Rosenbaum:2011}. But multineuronal recordings do provide more information than an equivalent number of separately recorded pairs of cells. For example, the eigenvalue decomposition of the covariance matrix expresses shared correlated activity components across the population; common fluctuations of population activity may be accurately represented by only a few eigenvectors that affect all correlation coefficients. On the other hand, a correlation matrix can be specified using the \emph{partial correlations} between pairs of the recorded neurons. The partial correlation coefficient between two neurons reflects their linear association conditioned on the activity of all the other recorded cells \cite{Whittaker:1990}.  Under some assumptions, partial correlations measure conditional independence between variables and may more directly approximate causal effects between components of complex systems than correlations \cite{Whittaker:1990}. For this reason, partial correlations have been used to describe interactions between genes in functional genomics \cite{Schafer:2005, Peng:2009} and between brain regions in imaging studies \cite{Varoquaux:2012, Ryali:2012}. These opportunities have not yet been explored in neurophysiological studies where most analyses have only considered the distributions of pairwise correlations \cite{Zohary:1994, Bair:2001, Smith:2008, Ecker:2010}.

However, estimation of correlation matrices from large populations presents a number of numerical challenges. The amount of recorded data grows only linearly with population size whereas the number of estimated coefficients increases quadratically. This mismatch leads to an increase in spurious correlations, overestimation of common activity (\emph{i.e.}\;overestimation of the largest eigenvalues) \cite{Ledoit:2004}, and poorly conditioned partial correlations \cite{Schafer:2005}. The \emph{sample correlation matrix} is an unbiased estimate of the true correlations but its many free parameters make it sensitive to sampling noise. As a result, on average, the sample correlation matrix is farther from the true correlation matrix than some structured estimates.

Estimation can be improved through \emph{regularization},  the technique of deliberately imposing a structure on an estimate in order to reduce its estimation error \cite{Schafer:2005, Bickel:2006}. To `impose a structure' on an estimate means to bias (`shrink') it toward a reduced representation  with fewer free parameters, the \emph{target estimate}.   The optimal target estimate and the optimal amount of shrinkage can be obtained from the data sample either analytically \cite{Ledoit:2003, Ledoit:2004, Schafer:2005}  or by cross-validation \cite{Friedman:1989}. An estimator that produces estimates that are, on average, closer to the truth for a given sample size is said to be more \emph{efficient} than other estimators.

Although regularized covariance matrix estimation is commonplace in finance \cite{Ledoit:2003}, functional genomics \cite{Schafer:2005}, and brain imaging \cite{Ryali:2012}, surprisingly little work has been done to identify optimal regularization of neural correlation matrices.

Improved estimation of the correlation matrix is beneficial in itself. For example, improved estimates can be used to optimize  decoding of the population activity \cite{Friedman:1989, Berens:2012}. But reduced estimation error is not the only benefit of regularization.  Finding the most efficient among many regularized estimators leads to insights about the system itself: the structure of the most efficient estimator is a parsimonious representation of the regularities in the data.

The advantages due to regularization increase with the size of the recorded population. With the advent of  big neural data \cite{Alivisatos:2013}, the search for optimal regularization schemes will become increasingly relevant in any model of population activity. Since optimal regularization schemes are specific to systems under investigation, the inference of functional connectivity in large-scale neural data will entail the search for optimal regularization schemes. Such schemes may involve combinations of heuristic rules and numerical techniques specially designed for given types of neural circuits.

What structures of correlation matrices best describe the multineuronal activity in specific circuits and in specific brain states?  More specifically, are correlations in the visual cortex during visual stimulation best explained by common fluctuations or by local interactions within the recorded microcircuit?

To address these questions, we evaluated four regularized covariance matrix estimators that imposed different structures on the estimate. The estimators are designated as follows:
\begin{description}
\item[$C_{\sf sample}$] -- the sample covariance matrix, the unbiased estimator.
\item[$C_{\sf diag}$] -- linear shrinkage of covariances toward zero, \emph{i.e.}\;toward a diagonal covariance matrix.
\item[$C_{\sf factor}$] -- a low-rank approximation of the sample covariance matrix, representing inputs from unobserved shared factors (latent units).
\item[$C_{\sf sparse}$] -- sparse partial correlations, \emph{i.e.}\;all but a small fraction of the \emph{partial} correlations between pairs of neurons are set to zero.
\item[$C_{\sf sparse+latent}$] -- sparse partial correlations between the recorded neurons \emph{and} linear interactions with a number of latent units.
\end{description}

First, we used simulated data to demonstrate that the selection of the optimal estimator indeed pointed to the true structure of the dependencies in the data.

We then performed a cross-validated evaluation to establish which of the four regularized estimators was most efficient for representing the population activity of dense groups of neurons in mouse primary visual cortex recorded with high-speed 3D random-access two-photon imaging of calcium signals. In our data, the sample correlation coefficients were largely positive and low.  We found that the most efficient estimate of the correlation matrix in these data was $C_{\sf sparse+latent}$.  This estimator revealed a sparse network of partial correlations (`interactions'), between the observed neurons; it also inferred a number of latent units interacting with the observed neurons. We analyzed these networks of partial correlations and found the following: Whereas significant noise correlations were predominantly positive, the inferred interactions had a large fraction of negative values possibly reflecting inhibitory circuitry.  Moreover, the inferred positive interactions exhibited a substantially stronger relationship to the physical distances and to the differences in preferred orientations than noise correlations. In contrast, the inferred negative interactions were less selective.

% Results and Discussion can be combined.
% We only support three levels of headings, please do not create a heading level below \subsubsection.
\section*{Results}
\subsection*{Covariance estimation}
The covariance matrix is defined as
\begin{equation}\label{eq:true-covariance}
    \Sigma = \E{(x-\mu)(x-\mu)^\T},\quad \mu = \E{x}
    \end{equation}
    where the $p\times 1$ vector $x$ is a single observation of the firing rates of $p$ neurons in a time bin of some duration, $\E{\cdot}$ denotes expectation, and $\mu$ is the vector of expected firing rates. 

Given a set of observations $\{x(t): t\in T$\} of population activity, where $x(t)$ contains observed firing rates in time bin $t$, and an independent estimate of the mean firing rates $\bar x$, the \emph{sample covariance matrix},
\begin{equation}\label{eq:sample}
    C_{\sf sample} = \frac 1 n \sum\limits_{t\in T} (x(t)-\bar x)(x(t)-\bar x)^\T,
    \end{equation}
where $n$ is the number of time bins in $T$, is an unbiased estimate of the true covariance matrix, \emph{i.e.} $\E{C_{\sf sample}}=\Sigma$. 
In all cases when the unbiasedness of the sample covariance matrix matters in this paper, the mean activity is estimated independently from a separate sample.

Given any covariance matrix estimate $C$, the corresponding correlation matrix $R$ is calculated by normalizing the rows and columns of $C$ by the square roots of its diagonal elements to produce unit entries on the diagonal:
\begin{equation}\label{eq:precision}
    R = (\mbox{diag}(C))^{-\frac 1 2} C (\mbox{diag}(C))^{-\frac 1 2},
\end{equation}
where $\mbox{diag}(C)$ denotes the diagonal matrix with the diagonal elements from $C$.

The \emph{partial correlation} between a pair of variables is the Pearson correlation coefficient of the residuals of the linear least-squares predictor of their activity based on all the other variables, excluding the pair \cite{Anderson:2003, Whittaker:1990}. Partial correlations figure prominently in probabilistic \emph{graphical modeling} wherein the joint distribution is explained by sets of pairwise interactions \cite{Whittaker:1990}. For multivariate Gaussian distributions, zero partial correlations indicate conditional independence of the pair, implying a lack of direct interaction \cite{Dempster:1972, Whittaker:1990}. More generally, partial correlations can serve as a measure of conditional independence under the assumption that dependencies in the system are close to linear effects \cite{Whittaker:1990,Baba:2004}. As neural recordings become increasingly dense, partial correlations may prove useful as indicators of conditional independence (lack of functional connectivity) between pairs of neurons.

Pairwise partial correlations are closely related to the elements of the \emph{precision matrix}, \emph{i.e.}\;the inverse of the covariance matrix \cite{Dempster:1972,Whittaker:1990}. Zero elements in the precision matrix signify zero partial correlations between the corresponding pairs of variables. Given the covariance estimate $C$, the matrix of partial correlations $P$ is computed by normalizing the rows and columns of the \emph{precision matrix} $C^{-1}$ to produce negative unit entries on the diagonal:
\begin{equation}\label{eq:partial}
    P = -\left(\mbox{diag}(C^{-1})\right)^{-\frac 1 2} C^{-1} \left(\mbox{diag}(C^{-1})\right)^{-\frac 1 2}
\end{equation}

Increasing the number of recorded neurons results in a higher \emph{condition number} of the sample covariance matrix \cite{Ledoit:2004} making the partial correlation estimates more \emph{ill-conditioned}: small errors in the covariance estimates translate into greater errors in the estimates of the partial correlations. With massively multineuronal recordings, partial correlations cannot be estimated without \emph{regularization} \cite{Ledoit:2004,Schafer:2005}.

We considered four regularized estimators based on distinct families of target estimates: $C_{\sf diag}$, $C_{\sf factor}$, $C_{\sf sparse}$, and $C_{\sf sparse+latent}$. In probabilistic models with exclusively linear dependencies, the target estimates of these estimators correspond to distinct families of graphical models (Fig.~\ref{fig:1} Row 1). 

The target estimate of estimator $C_{\sf diag}$ is the diagonal matrix $D$ containing estimates of neurons' variances. Regularization is achieved by linear \emph{shrinkage} of the sample covariance matrix $C_{\sf sample}$ toward $D$ as controlled by the scalar \emph{shrinkage intensity} parameter $\lambda \in [0, 1]$:
\begin{equation}\label{eq:c-diag}
C_{\sf diag} = (1-\lambda) C_{\sf sample} + \lambda D
\end{equation}
The structure imposed by $C_{\sf diag}$ favors (performs better  on) populations   with no linear associations between the neurons (Fig.~\ref{fig:1} Row 1, A).  If sample correlations are largely spurious, $C_{\sf diag}$ is expected to be more efficient than other estimators.

Estimator $C_{\sf factor}$ approximates the covariance matrix by the factor model, The estimator,
\begin{equation}\label{eq:c-factor}
C_{\sf factor} = L + D,
\end{equation}
where $L$ is a $p\times p$ symmetric positive semidefinite matrix with low rank and $D$ is a diagonal matrix. 
This approximation is the basis for \emph{factor analysis} \cite{Anderson:2003}, where matrix $L$ represents covariances arising from latent factors. The rank of $L$ corresponds to the number of latent factors. Matrix $D$ contains the variances of the cells' independent activity from the latent factors. The estimator is regularized by selecting the rank of $L$ and by shrinking the independent variances in $D$ toward their mean.
The structure imposed by $C_{\sf factor}$ favors conditions in which the population activity is linearly driven by a number of latent factors that affect many cells while direct interactions between the recorded cells are insignificant (Fig.~\ref{fig:1} Row 1, B).

Estimator $C_{\sf sparse}$ is produced by approximating the sample covariance matrix by the inverse of a sparse matrix $S$:
\begin{equation}\label{eq:c-sparse}
C_{\sf sparse} = S^{-1}.
\end{equation}
Here $S$ is a sparse matrix, \emph{i.e.}\;one in which a large fraction of off-diagonal elements are set to zero.  
The estimator is regularized by adjusting the sparsity (fraction of off-diagonal zeros) of $S$. 
The problem of finding the optimal set of non-zero elements in $S$ is known as \emph{covariance selection} \cite{Dempster:1972}. 
The structure imposed by $C_{\sf sparse}$ favors conditions in which neural correlations arise from direct linear effects (`interactions') between some pairs of neurons (Fig.~\ref{fig:1} Row 1, C).

Estimator $C_{\sf sparse+latent}$ is obtained by approximating the sample covariance matrix by a matrix whose inverse is the difference of a sparse component and a low-rank component:
\begin{equation}\label{eq:c-sl}
C_{\sf sparse+latent} = (S - L)^{-1},
\end{equation}
where $S$ is a sparse matrix and $L$ is a low-rank matrix. The estimator is regularized by adjusting the sparsity of $S$ and the rank of $L$. See Methods for more detailed explanations. The structure imposed by $C_{\sf sparse+latent}$ favors conditions in which the activity of neurons is determined by linear effects between some observed pairs of neurons and linear effects from several latent units (Fig.~\ref{fig:1} Row 1, D) \cite{Chandrasekaran:2010,Ma:2013}.

We refer to the sparse partial correlations in estimators $C_{\sf sparse}$ and $C_{\sf sparse+latent}$ as `interactions'.

\subsection*{Simulation}
We next demonstrated how the most efficient among different regularized estimators can reveal the structure of correlations.
We constructed four families of $50\times 50$ covariance matrices, each with structure that matched one of the four regularized estimators (Fig.~\ref{fig:1} Row 2, A--D and Methods).  We used these covariance matrices as the ground truth in multivariate Gaussian distributions with zero means and drew samples of various sizes. 
The sample correlation matrices from finite samples (\emph{e.g.}\ $n=500$ in Fig.~\ref{fig:1} Row 3) were contaminated with sampling noise and their underlying structures were difficult to discern.

The evaluation of any covariance matrix estimator, $C$, is performed with respect to a \emph{loss function} $\ell(C,\Sigma)$ to quantify its discrepancy from the truth, $\Sigma$.  The loss function is chosen to attain its minimum when $C=\Sigma$.
Here, in the role of the loss function we adopted the Kullback-Leibler divergence between multivariate normal distributions with equal means, scaled by $\frac 2 p$ to make its values comparable across different population sizes: 
\begin{equation}\label{eq:loss}
    \ell(C,\Sigma) = 
    \frac 2 p D_{KL}\left(\mathcal N_\Sigma\,\Vert\,\mathcal N_C\right) = 
    \frac 1 {p} \left[\Tr(C^{-1}\Sigma) + \ln\det C - \ln\det\Sigma - p\right] 
\end{equation}
Thus $\ell(C,\Sigma)$ is expressed in nats/neuron per time bin.

When the ground truth is not accessible, the loss cannot be computed directly but may be estimated from data through \emph{validation}.
In a validation procedure, a validation sample covariance matrix $C_{\sf sample}^\prime$ is computed from a testing data set that is independent from the data used for computing $C$.
Then the \emph{validation loss} $\loss{C,C^\prime_{\sf sample}}$ measures the discrepancy of $C$ from $C^\prime_{\sf sample}$. 
Here, in the role of validation loss, we adopted the negative multivariate normal log likelihood of $C$ given $C^\prime_{\sf sample}$, also scaled by $\frac 2 p$ and omitting the constant term: 
\begin{equation}\label{eq:vloss}
    \loss{C,C^\prime_{\sf sample}} = \frac 1 p \left[\Tr(C^{-1}C^\prime_{\sf sample})+\ln\det C \right]
\end{equation}

Since $\loss{\cdot,\cdot}$ is additive in its second argument and $C^\prime_{\sf sample}$ is an unbiased estimate of $\Sigma$, then, for given $C$ and $\Sigma$, the validation loss is an unbiased estimate of the true loss, up to a constant:
\begin{equation}
    \E{\loss{C,C_{\sf sample}^\prime}}=\loss{C,\E{C_{\sf sample}^\prime}}=\loss{C,\Sigma} = \ell(C,\Sigma) + \mbox{const}.
\end{equation}
Therefore, validation procedures allow comparing the relative values of the loss attained by different covariance matrix estimators.

We drew 30 independent samples with sample sizes $n=250$, 500, 1000, 2000, and 4000 from each model and computed the loss $\ell(C,\Sigma)$ for each of the five estimators.  
The hyperparameters of the regularized estimators were optimized by nested cross-validation using only the data in the sample.  
All the regularized estimators produced better estimates (lower loss) than the sample covariance matrix.  
However, estimators whose structure matched the true model outperformed the other estimators (Fig.~\ref{fig:1} Rows 4 and 5).
The validation loss computed by 10-fold cross-validation (see Methods) accurately reproduced the relative values of the true loss and the rankings of the estimators even without access to the ground truth (Fig.~\ref{fig:1} Row 6). 

Note that when the ground truth had zero correlations (Column A), $C_{\sf factor}$ performed equally well to $C_{\sf diag}$ because it correctly inferred zero factors and only estimated the individual variances. 
Similarly, when the number of latent units was zero (Column C), $C_{\sf sparse+latent}$ performed nearly equally well to $C_{\sf sparse}$ because it correctly inferred zero latent units.
With increasing sample sizes, all estimators converged to the ground truth (zero loss) but the estimators with correct structure outperformed the others even for large samples.

In Gaussian models, partial correlations perfectly characterize the conditional dependencies between variables and the graphical models of partial correlations exactly correspond to the conditional dependencies in the data. 
To demonstrate that estimator rankings were robust to deviations from Gaussian models, we repeated the same cross-validated evaluation using pairwise Ising models to generate the data.
Ising models have been used to infer functional connectivity from neuronal spike trains \cite{Hertz:2011}. 
Conveniently, the Ising model has equivalent mathematical form to the Gaussian distribution,
\begin{equation}\label{eq:ising}
    x\sim \frac 1 {Z(J,h)} \exp\left(\frac 1 2 x^\T J x + h^\T x \right)
\end{equation}
but the Ising model is defined on the multivariate binary domain rather than the continuous domain. 
Both models are maximum-entropy models constrained to match the mean firing rates and the covariance matrix \cite{Jaynes:1957}.
The partition function $Z(J,h)$ normalizes the distributions on the models' respective domains. 
In the Gaussian model, the matrix $-J^{-1}$ is the covariance matrix; and the mean values are $\mu=J^{-1}h$.  
For the Ising model, $J$ is the matrix of pairwise interactions and $h$ is the vector of the cells' individual activity drives, although they do not have a simple relationship to the means and the covariance matrix. 
Both distributions have the same structure of pairwise conditional dependencies: zeros in the matrix $J$ indicate conditional independence between the corresponding pair of neurons. 

Indeed, despite their considerable departure from strictly linear conditional dependencies, Ising models yielded the same relationships between the performances of the covariance estimators as the Gaussian models in cross-validation (Fig.~\ref{fig:2}). Identical interaction matrices $J$  of the joint distributions over the observable and latent variables were used for both the Gaussian and the Ising models.

This simulation study demonstrated that cross-validated evaluation of regularized estimators of the covariance matrices of population activity can discriminate between structures of dependencies in the population. The selection of the most efficient covariance estimators for particular neural circuits is therefore an empirical finding characteristic of the nature of circuit interactions.

\subsection*{The $C_{\sf sparse+latent}$ estimator is most efficient in neural data}
We recorded the calcium activity of densely sampled populations of neurons in layers 2/3 and upper layer 4 in primary visual cortex of sedated mice using fast random-access 3D scanning two-photon microscopy during visual stimulation (Fig.~\ref{fig:3} A--B) \cite{Reddy:2005, Katona:2012, Cotton:2013}. This technique allowed fast sampling (100--150 Hz) from large numbers (150--350) of cells in $200\times200\times100$ $\mu$m$^3$ volumes of cortical tissue (Fig.~\ref{fig:3} C and D).  The instantaneous firing rates were inferred using sparse nonnegative deconvolution \cite{Vogelstein:2010} (Fig.~\ref{fig:3} C). Only cells that produced detectable calcium activity were included in the analysis (see Methods).  First, 30 repetitions of full-field drifting gratings of 16 directions were presented in random order.  Each grating was played for 500 ms, without intervening blanks.  This stimulus was used to compute the orientation tuning of the recorded cells (Fig.~\ref{fig:3} D). To estimate the noise correlation matrix, we presented only two distinct directions in some experiments or five directions in others with 100--300 repetitions of each direction. Each grating lasted 1 second and was followed by a 1-second blank.  The traces were then binned into 150 ms intervals aligned on the stimulus onset for the estimation of the correlation matrix.   The sample correlation coefficients were largely positive and low (Fig.~\ref{fig:3} E and F). The average value of the correlation coefficient across sites ranged from 0.0065 to 0.051 with the mean across sites of 0.018.

In these densely sampled populations, direct interactions between cells are likely to influence the patterns of population activity.  We therefore hypothesized that covariance matrix estimators that explicitly modeled the partial correlations between pairs of neurons ($C_{\sf sparse}$ and $C_{\sf sparse+latent}$) would have a performance advantage.  However, the observed neurons must also be strongly influenced by global activity fluctuations and by unobserved common inputs to the advantage of estimators that explicitly model common fluctuations of the entire population: $C_{\sf factor}$ and $C_{\sf sparse+latent}$.  If both types of effects are significant, then $C_{\sf sparse+latent}$ should outperform the other estimators.

To test this hypothesis, we computed the validation loss of estimators $C_{\sf sample}$, $C_{\sf diag}$, $C_{\sf factor}$, $C_{\sf sparse}$, and $C_{\sf sparse+latent}$ in $n=27$ imaged sites in 14 mice.  The hyperparameters of each estimator were optimized by nested cross-validation (See Fig.~S1 and  Methods). Indeed, the sparse+latent estimator outperformed the other estimators (Fig.~\ref{fig:4}). The respective median differences of the validation loss were 0.039, 0.0016, 0.0029, and 0.0059 nats/cell/bin, significantly greater than zero ($p<0.01$ in each comparison, Wilcoxon signed rank test).

\subsection*{Structure of $C_{\sf sparse+latent}$ estimates}
We examined the composition of the $C_{\sf sparse+latent}$ estimates for each imaged site (Fig.~\ref{fig:5} and Fig.~\ref{fig:6}). Although the regularized estimates were similar to the sample correlation matrix (Fig.~\ref{fig:5} A and B), the corresponding partial correlation matrices differed substantially (Fig.~\ref{fig:5} C and D). The estimates separated two sources of correlations: a network of linear interactions expressed by the sparse component of the inverse and latent units expressed by the low-rank components of the inverse (Fig.~\ref{fig:5} E). The sparse partial correlations revealed a network that differed substantially from the network composed of the greatest coefficients in the sample correlation matrix (Fig.~\ref{fig:5} F, G, H, and I).

In the example site (Fig.~\ref{fig:5}), the sparse component had 92.8\% sparsity (or conversely, 7.2\% connectivity: $\mbox{connectivity}=1-\mbox{sparsity}$) with average node degree of 20.9 (Fig.~\ref{fig:5} G). The average node degree, \emph{i.e.}\;the average number of interactions linking each neuron, is related to connectivity as $\mbox{degree} = \mbox{connectivity}\cdot(p-1)$, where $p$ is the number of neurons. The low-rank component had rank 72, denoting 72 inferred latent units. The number of latent units increased with population size (Fig.~\ref{fig:6} A) but the connectivity was highly variable (Fig.~\ref{fig:6} B): Several sites, despite their large population sizes, were driven by latent units and had few pairwise interactions. This variability may be explained by differences in brain states and recording quality and warrants further investigation.

The average partial correlations calculated from these estimates according to Eq.~\ref{eq:partial} at all 27 sites were about 5 times lower than the average sample correlations (Fig.~\ref{fig:6} C). This suggests that correlations between neurons build up from multiple chains of smaller interactions. Furthermore, the average partial correlations were less variable ($p=0.002$ Brown-Forsythe test): the coefficient of variation of the average sample correlations across sites was 0.45 whereas that of the average partial correlations was 0.29.

While the sample correlations were mostly positive, the sparse component of the partial correlations (`interactions') had a high fraction (28.7\% in the example site) of negative values (Fig.~\ref{fig:5} F). The fraction of negative interactions increased with the inferred connectivity (Fig.~\ref{fig:6} D), suggesting that negative interactions can be inferred only after a sufficient density of positive interactions has been uncovered.

Thresholded sample correlations have been used in several studies to infer pairwise interactions \cite{Golshani:2009, Feldt:2011, Malmersjo:2013, Sadovsky:2014}.  We therefore compared the interactions in the sparse component of $C_{\sf sparse+latent}$ to those obtained from the sample correlations thresholded to the same level of connectivity. The networks revealed by the two methods differed substantially. In the example site with 7.2\% connectivity in $C_{\sf sparse+latent}$, only 27.7\% of the connections coincided with the above-threshold sample correlations (Fig.~\ref{fig:5} F, H, and I). In particular, most of the inferred negative interactions corresponded to low sample correlations (Fig.~\ref{fig:5} F) where high correlations are expected given the rest of the correlation matrix.

\subsection*{Relationship of $C_{\sf sparse+latent}$ to orientation tuning and physical distances}

We then examined how the structure of the $C_{\sf sparse+latent}$ estimates related to the differences in orientation preference and to the physical distances separating pairs of cells (Fig.\;\ref{fig:7}).  Five sites with highest pairwise connectivities were included in the analysis. Partial correlations were computed using Eq.~\ref{eq:partial} based on the regularized estimate, including both the sparse and the latent component. Connectivity was computed as the fraction of pairs of cells connected by non-zero elements (interactions) in the sparse component of the estimate, segregated into positive and negative connectivities.

First, we analyzed how correlations and connectivity depended on the differences in preferred orientations ($\Delta \mbox{ori}$) of pairs of significantly ($\alpha=0.05$) tuned cells. The partial correlations decayed more rapidly with $\Delta\mbox{ori}$ than did sample correlations ($p<10^{-9}$ in each of the five sites, two-sample $t$-test of the difference of the linear regression coefficients in normalized data). Positive connectivity decreased with $\Delta\mbox{ori}$ ($p<0.005$ in each of the five sites, $t$-test on the logistic regression coefficient) whereas negative connectivity did not decrease (Fig.~\ref{fig:7} D): The slope in the logistic model of connectivity with respect to $\Delta\mbox{ori}$ was significantly higher for positive than for negative interactions ($p<0.04$ in each of the five sites, two-sample $t$-test of the difference of the logistic regression coefficient).

Second, we compared how correlations and connectivity depended on the physical distance separating pairs of cells. We distinguished between lateral distance, $\Delta x$, in the plane parallel to the pia, and vertical distance, $\Delta z$, orthogonal to the pia.  When considering the dependence on $\Delta x$, the analysis was limited to cell pairs located at the same depth with $\Delta z < 30\,\mu\mbox{m}$; conversely, when considering the dependence on $\Delta z$, only vertically aligned cell pairs with $\Delta x < 30\,\mu\mbox{m}$ were included. Again, the partial correlations decayed more rapidly both laterally and vertically than sample correlations ($p<10^{-6}$ in each of the five sites, for both lateral and vertical distances, two-sample $t$-test of the difference of the linear regression coefficients).
Positive connectivity decayed with distance ($p<10^{-6}$ in each of the five sites for positive interactions and $p<0.05$ for negative interactions, $t$-test on the logistic regression coefficient in normalized data) (Fig.~\ref{fig:7} E), so that cells separated laterally by less than 25 $\mu\mbox{m}$ were 3.2 times more likely to be connected than cells separated laterally by more than 150 $\mu\mbox{m}$. Although the positive connectivity appeared to decay faster with vertical than with lateral distance, the differences in slopes of the respective logistic regression models were not significant with available data. The negative connectivity decayed slower with distance (Fig.~\ref{fig:7} E and F): The slope in the respective logistic models with respect to the lateral distance was significantly higher for positive than for negative connectivities ($p<0.05$ in each of the five sites, two-sample $t$-test of the difference of the logistic regression coefficients).

\section*{Discussion}
\subsection*{Functional connectivity as a network of pairwise interactions}
Functional connectivity is often represented as a graph of pairwise interactions. The goal of many studies of functional connectivity has been to estimate  anatomical connectivity from  observed multineuronal spiking activity.  For example, characteristic peaks and troughs in the pairwise cross-correlograms of recorded spike trains contain statistical signatures of monosynaptic connections and shared synaptic inputs \cite{Gerstein:1964, Perkel:1967, Moore:1970, Alonso:1998, Denman:2013}.  Such signatures are ambiguous as they can arise from network effects other than direct synaptic connections \cite{Aertsen:1989}.  With simultaneous recordings from more neurons, ambiguities can be resolved by inferring the conditional dependencies between pairs of neurons.  Direct causal interactions between neurons produce statistical dependency between them even after conditioning on the state of the remainder of the network and external input. Therefore, conditional independence shown statistically can signify the absence of a direct causal influence.

Conditional dependencies can be inferred by fitting a probabilistic model of the joint population activity. For example, generalized linear models (GLMs) have been constructed to  include biophysically plausible synaptic integration, membrane kinetics, and individual neurons' stimulus drive~\cite{Pillow:2008}.  Maximum entropy models constrained by observed pairwise correlations are among other models with pairwise coupling between cells \cite{Schneidman:2006, Tkacik:2006, Yu:2008, Tang:2008, Shlens:2009}.  Assuming that the population response follows a multivariate normal distribution, the conditional dependencies between pairs of neurons are expressed by the partial correlations between them.   Each probabilistic model, fitted to the same data may reveal a completely different network of `interactions',  \emph{i.e.}\;conditional dependencies between pairs of cells.

It is not yet clear which approach provides the best correspondence with anatomical connectivity. Little experimental evidence is available to answer this question.  The connectivity graphs inferred by various statistical methods are commonly reported without examining their relation to anatomy.
Topological properties of such graphs have been interpreted as principles of circuit organization (\emph{e.g.} small-world organization) \cite{Feldt:2011, Yu:2008, Malmersjo:2013, Sadovsky:2014}.  However, the topological properties of functional connectivity graphs can depend on the method of inference \cite{Zalesky:2012}. Until a physiological interpretation of functional connectivity is established, the physiological relevance of such analyses remains in question and we did not attempt applying graph-theoretical analyses to our results.

Inference of the conditional dependencies also depends on the completeness of the recorded population:  To equate conditional dependency to direct interaction between two neurons, we must record from all neurons with which the pair interacts. Unobserved portions of the circuit may manifest as conditional dependencies between observed neurons that do not interact. For this reason, statistical models of population activity have been most successfully applied to \emph{in vitro} preparations of the retina or cell cultures where high-quality recordings from the complete populations were available \cite{Pillow:2008}. In cortical tissue, electrode arrays record from a small fraction of cells in a given volume, limiting the validity of inference of the pairwise conditional dependencies. Perhaps for this reason, partial correlations have not, until now, been used to describe the functional connectivity in cortical populations.

Two-photon imaging of population calcium signals presents unique advantages for the estimation of functional connectivity.  While the temporal resolution of calcium signals is limited by the calcium dye kinetics, fast imaging techniques combined with spike inference algorithms provide millisecond-scale temporal resolution of single action potentials \cite{Grewe:2010}. However, such high temporal precision comes at the cost of the accuracy of inferred spike rates.  Better accuracy is achieved when calcium signals are analyzed on scales of tens of milliseconds \cite{Cotton:2013, Theis:2014}.  The major advantage of calcium imaging is its ability to characterize the spatial arrangement and types of recorded cells.  Recently, advanced imaging techniques have allowed recording from nearly every cell in a volume of cortical tissue  \emph{in vivo} \cite{Katona:2012, Cotton:2013} and even from entire nervous systems \cite{Leung:2013, Ahrens:2013}.  These techniques may provide more incisive measurements of functional connectivity than electrophysiological recordings.

The low temporal resolution of calcium signals limits the use of functional connectivity methods that rely on millisecond-scale binning of signals (cross-correlograms, some GLMs, and binary maximum entropy models).  Hence, most studies of functional connectivity have relied on instantaneous sample correlations \cite{Greenberg:2008, Golshani:2009, Hofer:2011, Malmersjo:2013} .  Although some investigators have interpreted such correlations as indicators of (chemical or electrical) synaptic connectivity, most used them as more general indicators of functional connectivity without relating them to underlying mechanisms.

In this study, we sought to infer pairwise functional connectivity networks  in cortical microcircuits. We hypothesized that partial correlations correspond more closely to underlying mechanisms than sample correlations when recordings are sufficiently dense.  Since neurons form synaptic connections mostly locally and sparsely \cite{Perin:2011}, we \emph{a priori} favored solutions with sparse partial correlations.  Under the assumptions that the recorded population is sufficiently complete and that the model correctly represents the nature of interactions, the network of partial correlations can better represent the  functional dependencies in the circuit than correlations.

\subsection*{Functional connectivity as coactivations}
Another approach to describing the functional connectivity of a circuit is to isolate individual patterns of multineuronal coactivations. Depending on the method of their extraction, coactivation patterns may be referred to as \emph{assemblies}, \emph{factor loadings}, \emph{principal components}, \emph{independent components}, \emph{activity modes}, \emph{eigenvectors}, or \emph{coactivation maps} \cite{Gerstein:1989, Chapin:1999, Peyrache:2010, Ch:2010, Lopes:2011, Lopes:2013}. Coactivation patterns could be interpreted as signatures of Hebbian cell assemblies, \emph{i.e.}\ groups of tightly interconnected groups of cells involved in a common computation \cite{Gerstein:1989, Ch:2010}.  Coactivation patterns could also result from shared input from unobserved parts of the circuit, or global network fluctuations modulating the activity of the local circuit \cite{Okun:2012, Ecker:2014}.

Coactivation patterns and pairwise connectivity are not mutually exclusive since assemblies arise from patterns of synaptic connectivity.  However, an analysis of coactivation shifts the focus from detailed interactions to  collective behavior.
In our study, the functional connectivity solely through modes of coactivations was represented by the factor analysis-based estimator $C_{\sf factor}$.

\subsection*{Combining pairwise interactions and coactivations}
In the effort to account for the joint activity patterns that are poorly explained by pairwise interactions, investigators have augmented models of pairwise interactions with additional mechanisms such as latent variables, higher-order correlations, or global network fluctuations \cite{Ganmor:2011, Tkacik:2013, Pfau:2013, Koster:2013, Ecker:2014}.

In our study, we combined pairwise interactions with collective coactivations by applying the recently developed numerical techniques for the inference of the partial correlation structure in systems with latent variables \cite{Chandrasekaran:2010, Ma:2013}.  The resulting estimator, $C_{\sf sparse+latent}$, effectively decomposed the functional connectivity into a sparse network of pairwise interactions and coactivation mode vectors.

\subsection*{Addressing ill-posedness}
Inferring the conditional dependencies between variables in a probabilistic model is an ill-posed problem: small variations in the data can produce very large errors in the inferred network of dependencies (Fig.~\ref{fig:5} C and D). The problem becomes worse as the number of  recorded neurons increases until such models lose their statistical validity \cite{Roudi:2009}.  As techniques have improved to allow recording from larger neuronal populations, experimental neuroscientists have addressed this problem by extending the recording durations to keep sampling noise in check and verified that existing models are not overfitted \cite{Tkacik:2013}. However, ambitious projects already underway, such as the BRAIN initiative  \cite{Alivisatos:2013}, aim to record from significantly larger populations. Simply increasing recording duration will not be practical or sufficient, and the problem must be addressed by using regularized estimators. Regularization biases the solution toward a small subspace in order to counteract the effects of  sampling noise in the empirical data. However, biasing the solution to an inappropriate subspace does not allow significant estimation improvement and hinders interpretation.

Several strategies have been developed to limit the model space in order to improve the quality of the estimate. For example, Ganmor et al. \cite{Ganmor:2011} developed a heuristic rule to identify the most significant features that must be fitted by a maximum entropy model for improved performance in the retina. As another example of regularization, generalized linear models typically employ $L_1$ penalty terms to constrain the solution space and to effectively reduce the dimensionality of the solution \cite{Pillow:2008}.

In our study, regularizations were accomplished by dimensionality reduction (feature selection) schemes to produce sparse, constrained solutions. Only the most efficient scheme was considered in the analysis of functional connectivity.

\subsection*{Model selection}
Various model selection criteria have been devised to select between families of models and the optimal subsets of variables in a given model family based on observed data. Despite its high computational demands, cross-validation is among the most popular model selection approaches due to its minimal assumptions about the data-generating process \cite{Arlot:2010}.

We evaluated the covariance matrix estimators using a loss function derived from the normal distribution.  However, this does not limit the applicability of its conclusions to normal distributions. Other probabilistic models, fitted to the same data, could also serve as estimators of the covariance matrix.  If a different model yields better estimation of the covariance matrix than the estimator proposed here, we believe that its structure should deserve consideration as the better representation of the functional connectivity.

The results of model selection must be interpreted with caution.  As we demonstrated by simulation, even models with incorrect forms of dependencies can substantially improve estimates (Fig.~\ref{fig:1}). Therefore, showing that a more constrained model has better cross-validated performance than a more complex model does not necessarily support the conclusion that it reveals a better representation of dependencies in the data.  This caveat is related to \emph{Stein's Paradox} \cite{Efron:1977}: The biasing of an estimate toward an arbitrary low-dimensional target can consistently outperform a less constrained estimate.

\subsection*{Physiological interpretation and future directions}

We showed that among several models a sparse network of linear interactions with several latent inputs yielded the best estimates of the noise covariance matrix for cortical microcircuits.  This finding is valuable in itself: improved estimates of the noise covariance matrix for large datasets are important in order to understand the role of noise correlations in population coding \cite{Abbott:1999, Sompolinsky:2001, Averbeck:2006, Josic:2009, Ecker:2011}

Moreover, this estimation approach provides a graphical representation of the dependencies in the data that can be used to formulate and test hypotheses about the structure of connectivity in the microcircuit. Importantly, the inferred functional interactions were substantially different from the network of the highest sample correlations.  For example, the $C_{\sf sparse+latent}$ estimator reveals a large number of negative interactions that were not present in the sample correlation matrix (Fig.~\ref{fig:5} F) and may reflect inhibitory circuitry.

Distances between cells in physical space and in sensory feature space had a stronger effect on the partial correlations estimated by the $C_{\sf sparse+latent}$ estimator than on sample correlations (Fig.~\ref{fig:7} A--C).
These differences support the idea that correlations are built up from partial correlations in chains of intermediate cells positioned closer and tuned more similarly to one another, with potentially closer correspondence to anatomical connectivity.  These differences may also be at least partially explained by a trivial effect of regularization: the $L_1$ penalty applied by the estimator (Eq.~\ref{eq:ma}) suppresses small partial correlations to greater extent than large partial correlations, enhancing the apparent effect of distance and tuning.  
Still, the distinct positive and negative connectivity patterns (Fig.\ \ref{fig:7} D--F)  may reflect geometric and graphical features of local excitatory and inhibitory networks. 
Indeed, the relationships between patterns of positive and negative connectivities inferred by the estimator resembled the properties of excitatory and inhibitory synaptic connectivities with respect to distance, cortical layers, and feature tuning \cite{Song:2005, Oswald:2008, Adesnik:2010, Perin:2011, Fino:2011, Hofer:2011, Isaacson:2011, Levy:2012}. For example, while excitatory neurons form synapses within highly specific local cliques \cite{Perin:2011}, inhibitory interneurons form synapses with nearly all excitatory cells within local microcircuits \cite{Fino:2011, Hofer:2011, Packer:2011}.  To further investigate the link between synaptic connectivity and inferred functional connectivity, in future experiments, we will use molecular markers for various cell types with follow-up multiple whole-cell \emph{in vitro} recordings \cite{Hofer:2011, Ko:2013} to directly compare the inferred functional connectivity graphs to the underlying anatomical circuitry. Finally, the latent units inferred by the estimator can be analyzed for their physiological functions. For example, these latent units may be modulated under different brain states (e.g. slow-wave sleep, attention) and stimulus conditions (e.g. certain types of stimuli may engage feedback connections) \cite{Reimer:2014,Fu:2014}.


% You may title this section "Methods" or "Models". 
% "Models" is not a valid title for PLoS ONE authors. However, PLoS ONE
% authors may use "Analysis" 
\section*{Materials and Methods}

\subsection*{Ethics statement}
All procedures were conducted in accordance with the ethical guidelines of the National Institutes of Health and were approved by the Baylor College of Medicine IACUC.

\subsection*{Surgery and two-photon imaging}
The surgical procedures and data acquisition were performed as described in \cite{Cotton:2013}: C57BL/6J mice (aged p40--60) were used. For surgery, animals were initially anesthetized with isoflurane (3\%). During the experiments, animals were sedated with a mixture of fentanyl (0.05 mg/kg), midazolam (5 mg/kg), and medetomidine (0.5 mg/kg), with boosts of half the initial dose every 3 hours.  A craniotomy was performed over the right primary visual cortex.  Membrane-permeant calcium indicator Oregon Green 488 BAPTA-1 AM (OGB-1, Invitrogen) was loaded by bolus injection.  The craniotomy was sealed using a glass coverslip secured with dental cement.

Calcium imaging began 1 hour after dye injection.  All imaging was performed using 3D-RAMP two-photon microscopy \cite{Cotton:2013}. First, a 3D stack was acquired and cells were manually segmented. Then calcium signal were collected by sampling in the center of each cell at rates of 100 Hz or higher, depending on the number of cells.

\subsection*{Visual stimulus}
The visual stimulus consisted of full-field drifting gratings with 90\% contrast, 10 cd/m$^2$ luminance, 0.08 cycles/degree spatial frequency, and 2 cycles/s temporal frequency. Two types of stimuli were presented for each imaging site: First, directional tuning was mapped using a pseudo-random sequence of drifting gratings at sixteen directions of motion, 500 ms per direction, without blanks, with 12--30 trials for each direction of motion.  Second, to measure correlations, the stimulus was modified to include only two directions of motion (in 9 datasets) or five directions (in 22 datasets) and the gratings were presented for 1 second and were separated by 1-second blanks, with 100--300 trials for each direction of motion.

\subsection*{Data processing}
All data were processed in MATLAB using the DataJoint data processing chain toolbox (http://datajoint.github.com).

The measured fluorescent traces were deconvolved to reconstruct the firing rates for each neuron: First, the first principal component was subtracted from the raw traces in order to reduce common mode noise related to small cardiovascular movements \cite{Cotton:2013}. The resulting traces were high-pass filtered above 0.1 Hz and downsampled to 20 Hz (Fig.~\ref{fig:3} C). Then, the firing rates were estimated using by nonnegative deconvolution \cite{Vogelstein:2010}.

Orientation tuning was computed by fitting the mean firing rates for each direction of motion $\phi$ using two-peaked von Mises tuning functions $f(\phi)=a + b\exp\left[\frac 1 w(\cos(\phi-\theta)-1) \right] + c\exp\left[\frac 1 w(\cos(\phi-\theta+\pi)-1) \right]$ where $b\ge c$ are the amplitudes of the two respective peaks, $w$ is the tuning width, and  $\theta$ is the preferred direction. The significance of the fit was determined by the permutation test: the labels of the direction were randomly permuted 10,000 times; the $p$-values of the fits were computed as the fraction of permutations that yielded $R^2$ equal to or higher than that of the original data.  Cells were considered tuned with $p<0.05$.

For covariance estimation, the analysis was limited to the period with 2 or 5 stimulus conditions and lasted between 14 and 27 minutes (mean 22 minutes).  Cells that did not have substantial spiking activity (those whose variance was less than 1\% of the median across the site) or whose activity was unstable (those whose variance in the least active quarter of the recording did not exceed 1\% of the variance in the most active quarter) were excluded from the analysis.

\subsection*{Cross-validation}
To compare the performance of the estimators, we used conventional 10-fold cross-validation: Trials were randomly divided into 10 subsets with approximately equal numbers of trials of each condition in each subset. Each subset was then used as the testing sample with the rest of the data used as the training sample for estimating the covariance matrix. The average validation loss over the 10 folds was reported.

Since each of the regularized estimators had one or two hyperparameters, we used \emph{nested cross-validation}:  The outer loop evaluated the performance of the estimators with the hyperparameter values optimized by cross-validation within the inner loop.  Hyperparameters were optimized by a two-phase search algorithm: random search to find a good starting point for the subsequent pattern search to find the global minimum.  The inner cross-validation loop subdivided the training dataset from the outer loop to perform 10-fold cross-validation in order to evaluate each choice of the hyperparameter values.  Thus the size of the training dataset within the inner loop comprised 81\% of the entire recording. Fig.~S1 illustrates the dependence of the validation loss on the hyperparameters of the $C_{\sf sparse+latent}$ estimator for the example site shown in Figures \ref{fig:3} and \ref{fig:5} and the optimal value found by the pattern search algorithm.

When the validation loss was not required, only the inner loop of cross-validation was used on the entire dataset.  This approach was used to compute the covariance matrix estimates and their true loss in the simulation study (Fig.~\ref{fig:1} Rows 4 and 5) and to analyze the partial correlation structure of the $C_{\sf sparse+latent}$ estimator (Fig.~\ref{fig:5}--\ref{fig:7}).

\subsection*{Covariance estimation}
Within the inner loop of cross-validation, regularized covariance matrix estimation required only the sample covariance matrix $C_{\sf sample}$ of the training dataset and the hyperparameter values provided by the outer loop.

Estimator $C_{\sf diag}$ (Eq.~\ref{eq:c-diag})  used two hyperparameters: the covariance shrinkage intensity $\lambda \in [0,1]$ and variance shrinkage intensity $\alpha \in [0,1]$.  The variances (the diagonal of $C_{\sf sample}$) were shrunk linearly toward their mean value $\frac 1 p \Tr(C_{\sf sample})$:
\begin{equation}\label{eq:shrink}
D = (1-\alpha)\mbox{diag}(C_{\sf sample}) + \alpha \frac 1 p \Tr(C_{\sf sample}) I
\end{equation}
The $C_{\sf diag}$ estimate was then obtained by shrinking $C_{\sf sample}$ toward $D$ according to Eq.~\ref{eq:c-diag}.

In estimator $C_{\sf factor}$ (Eq.~\ref{eq:c-factor}), the low-rank matrix $L$ and the diagonal matrix $D$ were found by solving the minimization problem
\begin{equation}
(L,D) = \argmin\limits_{\hat L,\hat D} \loss{\hat L + \hat D,C_{\sf sample}},
\end{equation}
using an expectation-maximization (EM) algorithm for a specified rank of $L$. After that, the diagonal of $D$ was linearly shrunk toward the its mean diagonal value similar to Eq.~\ref{eq:shrink}.

In estimator $C_{\sf sparse}$ (Eq.~\ref{eq:c-sparse}), the sparse precision matrix $S$ was found by minimizing the $L_1$-penalized loss with regularization parameter $\lambda$:
\begin{equation}
S = \argmin\limits_{\hat S \succ 0} \loss{{\hat S}^{-1},C_{\sf sample}} + \lambda \|\hat S \|_1
\end{equation}
where $\hat S\succ 0$ denotes the constraint that $\hat S$ be a positive-definite matrix and $\|\hat S\|_1$ is the element-wise $L_1$ norm of the matrix $\hat S$. This problem formulation is known as \emph{graphical lasso} \cite{Meinshausen:2006, Friedman:2008}. To solve this minimization problem, we adapted the alternative-direction method of multipliers (ADMM) \cite{Ma:2013}.
Unlike $C_{\sf diag}$ and $C_{\sf factor}$, this estimator does not include linear shrinkage: the selection of the sparsity level provides sufficient flexibility to fine-tune the regularization level.

Estimator $C_{\sf sparse+latent}$ (Eq.~\ref{eq:c-sl}) estimates a larger sparse precision matrix $S^\ast$ of the joint distribution of the $p$ observed neurons and $d$ latent units.
\begin{equation}
S^\ast=
\begin{pmatrix}
S & S_{12} \\
S_{12}^\T & S_{22}
\end{pmatrix},
\end{equation}
where the $p\times p$ partition $S$ corresponds to the visible units.
Then the covariance matrix of the observed population is
\begin{equation}
C_{\sf sparse+latent} = \left(S-S_{12}S_{22}^{-1}S_{12}^\T\right)^{-1}
\end{equation}
The rank of the $p\times p$  matrix $L=S_{12}S_{22}^{-1}S_{12}^\T$ matches the number of the latent units in the joint distribution. Rather than finding $S_{12}$ and $S_{22}$ separately, $L$ can be estimated as a low-rank positive semidefinite matrix. To simultaneously optimize the sparse component $S$ and the low-rank component $L$, we adapted the loss function with an $L_1$ penalty on $S$ and another penalty on the trace of $L$ \cite{Chandrasekaran:2010,Ma:2013}:
\begin{equation}\label{eq:ma}
	(S,L) = \argmin\limits_{\hat S,\hat L} \loss{(\hat S-\hat L)^{-1}, C_{\sf sample}} + \alpha\|\hat S\|_1 + \beta\Tr(\hat L)
\end{equation}
The trace of a symmetric semidefinite matrix equals the sum of the absolute values of its eigenvalues, \emph{i.e.} its \emph{nuclear norm}; penalty on $\Tr(L)$ favors solutions with few non-zero eigenvalues or, equivalently, low-rank solutions while keeping the convexity of the overall optimization problem \cite{Fazel:2002,Recht:2010}. This allows using convex optimization algorithm such as ADMM to be applied with great computational efficiency \cite{Ma:2013}.

The partial correlation matrix (Eq.~\ref{eq:partial}) computed from $C_{\sf sparse+latent}$ includes interactions between the visible and latent units and was used in Fig.~\ref{fig:5} C and D and Fig.~\ref{fig:6} C, and Fig.~\ref{fig:7} A--C).  The partial correlation matrix computed from $S$ alone expresses strengths of pairwise interactions
\begin{equation}
P_{\sf sparse} = -(\mbox{diag}(S))^{-\frac 1 2} S  (\mbox{diag}(S))^{-\frac 1 2}
\end{equation}
and were used in Fig.~\ref{fig:5} F, G, H.

The MATLAB code for these computations is available online at http://github.com/atlab/cov-est.
\subsection*{Cross-validation with conditioned variances}
Special attention was given to estimating the variances.  
All evaluations and optimization in this study were defined with respect to the covariance matrices.  
However, neuroscientists often estimate a common correlation matrix across multiple stimulus conditions when the variances of responses are conditioned on the stimulus \cite{Vogels:1989, Ponce:2013}. In this study, we too conditioned the variances on the stimulus but estimated a single correlation matrix across all conditions.
Here we describe the computation of the validation loss (Eq.~\ref{eq:vloss}) when the variances were allowed to vary with the stimulus condition.

Let $T_c$ and $T_c^\prime$ denote the sets of time bin indices for the training and testing samples, respectively, limited to condition $c$. 

Similar to Eq.~\ref{eq:sample}, the training and testing sample covariance matrices for condition $c$ are
\begin{equation}
    C_{c,{\sf sample}}
    = \frac 1{n_c} \sum\limits_{t\in T_c}\left(x(t)-\bar x_c\right)\left(x(t)-\bar x_c\right)^\T
\end{equation}
and
\begin{equation}
    C_{c,{\sf sample}}^\prime
    = \frac 1{n_c^\prime} \sum\limits_{t\in T_c^\prime}\left(x(t)-\bar x_c\right)\left(x(t)-\bar x_c\right)^\T
\end{equation}
Here $n_c$ and $n_c^\prime$ denote the sizes of $T_c$ and $T_c^\prime$, respectively.

Note that $\bar x_c= \frac 1 {n_c} \sum\limits_{t \in T_c}x(t)$ is estimated from the training sample but used in both estimates, making $C_{c,\sf sample}^\prime$ an unbiased estimate of the true covariance matrix, $\Sigma$. As such, $C_{c,\sf sample}^\prime$ can be used for validation.

The common correlation matrix $R_{\sf sample}$ is estimated by averaging the condition-specific correlations:
\begin{equation}
    R_{\sf sample}  
    = \frac 1 n \sum\limits_c n_c \left(V_{c,\sf sample}^{-\frac 1 2}C_{c,\sf sample}V_{c,\sf sample}^{-\frac 1 2}\right)
    = \frac 1 n \sum\limits_c \sum\limits_{t \in T_c} z(t)z(t)^\T,
\end{equation}
where $n=\sum\limits_c n_c$ and $V_{c,\sf sample} = \mbox{diag}(C_{c,\sf sample})$ is the diagonal matrix containing the sample variances. Then $R_{\sf sample}$ is simply the covariance matrix of the $z$-score signal $z(t) = V_{c,\sf sample}^{-\frac 1 2} \left(x(t) - \bar x_c\right)$ of the training sample.

For consistency with prior work, we applied regularization to covariance matrices rather than to correlation matrices. The common covariance matrix was estimated by scaling $R_{\sf sample}$ by the average variances across conditions $V_{\sf sample} = \frac 1 n \sum\limits_c n_c V_{c,\sf sample}$:
\begin{equation}
    C_{\sf sample} = V_{\sf sample}^{\frac 1 2} R_{\sf sample} V_{\sf sample}^{\frac 1 2}
\end{equation}
Note that $C_{\sf sample}$ differs from the sample covariance matrix computed without conditioning the variances on $c$ and this computation helps avoid any biases that would be introduced by ignoring changes in variance.

The covariance matrix estimators $C_{\sf diag}$, $C_{\sf factor}$, $C_{\sf sparse}$ or $C_{\sf sparse+latent}$ convert $C_{\sf sample}$ into its regularized counterpart denoted here as $C_{\sf reg}$.

To evaluate the estimators, we regularized the conditioned variances by linear shrinkage toward their mean value across all conditions. This was done by scaling $C_{\sf reg}$ by the conditioned variance adjustment matrix $Q_c = \delta I + (1-\delta)V_{\sf sample}^{-1} V_{c,\sf sample}$ to produce the conditioned regularized covariance matrix estimate:
\begin{equation}
    C_{c,\sf reg} = Q_c^{\frac 1 2} C_{\sf reg} Q_c^{\frac 1 2}
\end{equation}

The variance regularization parameter $\delta \in [0,1]$ was optimized in the inner loop of cross-validation along with the other hyperparameters.

The overall validation loss is obtained by averaging the validation losses across all conditions:
\begin{equation}\label{eq:full-loss}
    \frac 1 {\sum\limits_c n_c^\prime}\sum\limits_c n_c^\prime \loss{C_{c,\sf reg}, C_{c,\sf sample}^\prime} 
\end{equation}

With negative normal log-likelihood as the validation loss  (Eq.~\ref{eq:vloss}) and the unbiased validation covariance matrix $C_{c,\sf sample}$, the loss function in Eq.~\ref{eq:full-loss} is an unbiased estimate of the true loss. Hence, it was used for evaluations reported in Fig.~\ref{fig:4}.

\subsection*{Simulation}
For simulation, ground truth covariance matrices were produced by taking 150 independent samples from an artificial population of 50 independent, identically normally distributed units. The covariance matrices were then subjected to the respective regularizations to produce the ground truth matrices for the simulation studies (Fig.~\ref{fig:1} Row 2). Samples were then drawn from multivariate normal distributions models with the respective true covariance matrices to be estimated by each of the estimators. For Ising models, the negative inverse of the true covariance matrix was used as the matrix of coupling coefficients and the sampling was performed by the Metropolis-Hastings algorithm.

% Do NOT remove this, even if you are not including acknowledgments.

\section*{Acknowledgments}
We thank Genevera Allen for a helpful discussion, and Eftychios Pnevmatikakis for helpful suggestions and feedback on the manuscript.  

%\section*{References}
% Either type in your references using
% \begin{thebibliography}{}
% \bibitem{}
% Text
% \end{thebibliography}
%
% OR
%
% Compile your BiBTeX database using our plos2009.bst
% style file and paste the contents of your .bbl file
% here.
% 
\begin{thebibliography}{100}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi:\discretionary{}{}{}#1}\else
  \providecommand{\doi}{doi:\discretionary{}{}{}\begingroup
  \urlstyle{rm}\Url}\fi
\providecommand{\bibAnnoteFile}[1]{%
  \IfFileExists{#1}{\begin{quotation}\noindent\textsc{Key:} #1\\
  \textsc{Annotation:}\ \input{#1}\end{quotation}}{}}
\providecommand{\bibAnnote}[2]{%
  \begin{quotation}\noindent\textsc{Key:} #1\\
  \textsc{Annotation:}\ #2\end{quotation}}
\providecommand{\eprint}[2][]{\url{#2}}

\bibitem{Averbeck:2006}
Averbeck BB, Latham PE, Pouget A (2006) Neural correlations, population coding
  and computation.
\newblock Nat Rev Neurosci 7: 358--366.
\bibAnnoteFile{Averbeck:2006}

\bibitem{Zohary:1994}
Zohary E, Shadlen MN, Newsome WT (1994) Correlated neuronal discharge rate and
  its implications for psychophysical performance.
\newblock Nature 370: 140--143.
\bibAnnoteFile{Zohary:1994}

\bibitem{Kohn:2005}
Kohn A, Smith MA (2005) Stimulus dependence of neuronal correlation in primary
  visual cortex of the macaque.
\newblock J Neurosci 25: 3661-73.
\bibAnnoteFile{Kohn:2005}

\bibitem{Bair:2001}
Bair W, Zohary E, Newsome WT (2001) Correlated firing in macaque visual area
  mt: time scales and relationship to behavior.
\newblock The journal of Neuroscience 21: 1676--1697.
\bibAnnoteFile{Bair:2001}

\bibitem{Ecker:2010}
Ecker AS, Berens P, Keliris GA, Bethge M, Logothetis NK, et~al. (2010)
  Decorrelated neuronal firing in cortical microcircuits.
\newblock Science 327: 584-587.
\bibAnnoteFile{Ecker:2010}

\bibitem{Abbott:1999}
Abbott L, Dayan P (1999) The effect of correlated variability on the accuracy
  of a population code.
\newblock Neural computation 11: 91--101.
\bibAnnoteFile{Abbott:1999}

\bibitem{Sompolinsky:2001}
Sompolinsky H, Yoon H, Kang K, Shamir M (2001) Population coding in neuronal
  systems with correlated noise.
\newblock Physical Review E 64: 051904.
\bibAnnoteFile{Sompolinsky:2001}

\bibitem{Nirenberg:2003}
Nirenberg S, Latham PE (2003) Decoding neuronal spike trains: How important are
  correlations?
\newblock Proceedings of the National Academy of Sciences 100: 7348--7353.
\bibAnnoteFile{Nirenberg:2003}

\bibitem{Josic:2009}
Josic K, Shea-Brown E, Doiron B, de~la Rocha J (2009) Stimulus-dependent
  correlations and population codes.
\newblock Neural computation 21: 2774--2804.
\bibAnnoteFile{Josic:2009}

\bibitem{Berens:2011}
Berens P, Ecker AS, Gerwinn S, Tolias AS, Bethge M (2011) Reassessing optimal
  neural population codes with neurometric functions.
\newblock Proc Natl Acad Sci U S A 108: 4423-8.
\bibAnnoteFile{Berens:2011}

\bibitem{Ecker:2011}
Ecker AS, Berens P, Tolias AS, Bethge M (2011) The effect of noise correlations
  in populations of diversely tuned neurons.
\newblock The Journal of Neuroscience 31: 14272--14283.
\bibAnnoteFile{Ecker:2011}

\bibitem{Gerstein:1964}
Gerstein G, Clark W (1964) Simultaneous studies of firing patterns in several
  neurons.
\newblock Science 143: 1325--1327.
\bibAnnoteFile{Gerstein:1964}

\bibitem{Smith:2008}
Smith MA, Kohn A (2008) Spatial and temporal scales of neuronal correlation in
  primary visual cortex.
\newblock J Neurosci 28: 12591--12603.
\bibAnnoteFile{Smith:2008}

\bibitem{Denman:2013}
Denman DJ, Contreras D (2013) The structure of pairwise correlation in mouse
  primary visual cortex reveals functional organization in the absence of an
  orientation map.
\newblock Cereb Cortex .
\bibAnnoteFile{Denman:2013}

\bibitem{Ko:2011}
Ko H, Hofer SB, Pichler B, Buchanan KA, Sj{\"o}str{\"o}m PJ, et~al. (2011)
  Functional specificity of local synaptic connections in neocortical networks.
\newblock Nature 473: 87--91.
\bibAnnoteFile{Ko:2011}

\bibitem{Arieli:1995}
Arieli A, Shoham D, Hildesheim R, Grinvald A (1995) Coherent spatiotemporal
  patterns of ongoing activity revealed by real-time optical imaging coupled
  with single-unit recording in the cat visual cortex.
\newblock Journal of Neurophysiology 73: 2072--2093.
\bibAnnoteFile{Arieli:1995}

\bibitem{Chiu:2002}
Chiu C, Weliky M (2002) Relationship of correlated spontaneous activity to
  functional ocular dominance columns in the developing visual cortex.
\newblock Neuron 35: 1123--1134.
\bibAnnoteFile{Chiu:2002}

\bibitem{Kenet:2003}
Kenet T, Bibitchkov D, Tsodyks M, Grinvald A, Arieli A (2003) Spontaneously
  emerging cortical representations of visual attributes.
\newblock Nature 425: 954--956.
\bibAnnoteFile{Kenet:2003}

\bibitem{Cohen:2008}
Cohen MR, Newsome WT (2008) Context-dependent changes in functional circuitry
  in visual area mt.
\newblock Neuron 60: 162--173.
\bibAnnoteFile{Cohen:2008}

\bibitem{Cohen:2009}
Cohen MR, Maunsell JH (2009) Attention improves performance primarily by
  reducing interneuronal correlations.
\newblock Nature neuroscience 12: 1594--1600.
\bibAnnoteFile{Cohen:2009}

\bibitem{Rothschild:2010}
Rothschild G, Nelken I, Mizrahi A (2010) Functional organization and population
  dynamics in the mouse primary auditory cortex.
\newblock Nature neuroscience 13: 353--360.
\bibAnnoteFile{Rothschild:2010}

\bibitem{Smith:2013b}
Smith MA, Sommer MA (2013) Spatial and temporal scales of neuronal correlation
  in visual area v4.
\newblock The Journal of Neuroscience 33: 5422--5432.
\bibAnnoteFile{Smith:2013b}

\bibitem{Hofer:2011}
Hofer SB, Ko H, Pichler B, Vogelstein J, Ros H, et~al. (2011) Differential
  connectivity and response dynamics of excitatory and inhibitory neurons in
  visual cortex.
\newblock Nature neuroscience 14: 1045--1052.
\bibAnnoteFile{Hofer:2011}

\bibitem{Hansen:2012}
Hansen BJ, Chelaru MI, Dragoi V (2012) Correlated variability in laminar
  cortical circuits.
\newblock Neuron 76: 590--602.
\bibAnnoteFile{Hansen:2012}

\bibitem{Smith:2013}
Smith MA, Jia X, Zandvakili A, Kohn A (2013) Laminar dependence of neuronal
  correlations in visual cortex.
\newblock Journal of neurophysiology 109: 940--947.
\bibAnnoteFile{Smith:2013}

\bibitem{Golshani:2009}
Golshani P, Gon{\c c}alves JT, Khoshkhoo S, Mostany R, Smirnakis S, et~al.
  (2009) Internally mediated developmental desynchronization of neocortical
  network activity.
\newblock J Neurosci 29: 10890-9.
\bibAnnoteFile{Golshani:2009}

\bibitem{Gu:2011}
Gu Y, Liu S, Fetsch CR, Yang Y, Fok S, et~al. (2011) Perceptual learning
  reduces interneuronal correlations in macaque visual cortex.
\newblock Neuron 71: 750--761.
\bibAnnoteFile{Gu:2011}

\bibitem{Ko:2013}
Ko H, Cossell L, Baragli C, Antolik J, Clopath C, et~al. (2013) The emergence
  of functional microcircuits in visual cortex.
\newblock Nature 496: 96--100.
\bibAnnoteFile{Ko:2013}

\bibitem{Greenberg:2008}
Greenberg DS, Houweling AR, Kerr JN (2008) Population imaging of ongoing
  neuronal activity in the visual cortex of awake rats.
\newblock Nature neuroscience 11: 749--751.
\bibAnnoteFile{Greenberg:2008}

\bibitem{Goard:2009}
Goard M, Dan Y (2009) Basal forebrain activation enhances cortical coding of
  natural scenes.
\newblock Nat Neurosci 12: 1444-9.
\bibAnnoteFile{Goard:2009}

\bibitem{Kohn:2009}
Kohn A, Zandvakili A, Smith MA (2009) Correlations and brain states: from
  electrophysiology to functional imaging.
\newblock Curr Opin Neurobiol 19: 434-8.
\bibAnnoteFile{Kohn:2009}

\bibitem{Ecker:2014}
Ecker AS, Berens P, Cotton RJ, Subramaniyan M, Denfield GH, et~al. (2014) State
  dependence of noise correlations in macaque primary visual cortex.
\newblock Neuron 82: 235--248.
\bibAnnoteFile{Ecker:2014}

\bibitem{Renart:2010}
Renart A, de~la Rocha J, Bartho P, Hollender L, Parga N, et~al. (2010) The
  asynchronous state in cortical circuits.
\newblock Science 327: 587-90.
\bibAnnoteFile{Renart:2010}

\bibitem{Perkel:1967}
Perkel DH, Gerstein GL, Moore GP (1967) Neuronal spike trains and stochastic
  point processes: {II}. simultaneous spike trains.
\newblock Biophysical journal 7: 419--440.
\bibAnnoteFile{Perkel:1967}

\bibitem{Moore:1970}
Moore GP, Segundo JP, Perkel DH, Levitan H (1970) Statistical signs of synaptic
  interaction in neurons.
\newblock Biophysical Journal 10: 876--900.
\bibAnnoteFile{Moore:1970}

\bibitem{Shadlen:1998}
Shadlen MN, Newsome WT (1998) The variable discharge of cortical neurons:
  implications for connectivity, computation, and information coding.
\newblock J Neurosci 18: 3870-96.
\bibAnnoteFile{Shadlen:1998}

\bibitem{Salinas:2001}
Salinas E, Sejnowski TJ (2001) Correlated neuronal activity and the flow of
  neural information.
\newblock Nature Reviews Neuroscience 2: 539--550.
\bibAnnoteFile{Salinas:2001}

\bibitem{Ostojic:2009}
Ostojic S, Brunel N, Hakim V (2009) How connectivity, background activity, and
  synaptic properties shape the cross-correlation between spike trains.
\newblock The Journal of Neuroscience 29: 10234--10253.
\bibAnnoteFile{Ostojic:2009}

\bibitem{Rosenbaum:2011}
Rosenbaum R, Josi{\'c} K (2011) Mechanisms that modulate the transfer of
  spiking correlations.
\newblock Neural computation 23: 1261--1305.
\bibAnnoteFile{Rosenbaum:2011}

\bibitem{Whittaker:1990}
Whittaker J (1990) Graphical models in applied multivariate statistics.
\newblock Wiley Publishing.
\bibAnnoteFile{Whittaker:1990}

\bibitem{Schafer:2005}
Sch{\"a}fer J, Strimmer K, et~al. (2005) A shrinkage approach to large-scale
  covariance matrix estimation and implications for functional genomics.
\newblock Statistical applications in genetics and molecular biology 4: 32.
\bibAnnoteFile{Schafer:2005}

\bibitem{Peng:2009}
Peng J, Wang P, Zhou N, Zhu J (2009) Partial correlation estimation by joint
  sparse regression models.
\newblock Journal of the American Statistical Association 104.
\bibAnnoteFile{Peng:2009}

\bibitem{Varoquaux:2012}
Varoquaux G, Gramfort A, Poline JB, Thirion B (2012) Markov models for fmri
  correlation structure: is brain functional connectivity small world, or
  decomposable into networks?
\newblock Journal of Physiology-Paris 106: 212--221.
\bibAnnoteFile{Varoquaux:2012}

\bibitem{Ryali:2012}
Ryali S, Chen T, Supekar K, Menon V (2012) Estimation of functional
  connectivity in fmri data using stability selection-based sparse partial
  correlation with elastic net penalty.
\newblock Neuroimage 59: 3852--3861.
\bibAnnoteFile{Ryali:2012}

\bibitem{Ledoit:2004}
Ledoit O, Wolf M (2004) A well-conditioned estimator for large-dimensional
  covariance matrices.
\newblock Journal of multivariate analysis 88: 365--411.
\bibAnnoteFile{Ledoit:2004}

\bibitem{Bickel:2006}
Bickel PJ, Li B, Tsybakov AB, van~de Geer SA, Yu B, et~al. (2006)
  Regularization in statistics.
\newblock Test 15: 271--344.
\bibAnnoteFile{Bickel:2006}

\bibitem{Ledoit:2003}
Ledoit O, Wolf M (2003) Improved estimation of the covariance matrix of stock
  returns with an application to portfolio selection.
\newblock Journal of Empirical Finance 10: 603--621.
\bibAnnoteFile{Ledoit:2003}

\bibitem{Friedman:1989}
Friedman JH (1989) Regularized discriminant analysis.
\newblock Journal of the American statistical association 84: 165--175.
\bibAnnoteFile{Friedman:1989}

\bibitem{Berens:2012}
Berens P, Ecker AS, Cotton RJ, Ma WJ, Bethge M, et~al. (2012) A fast and simple
  population code for orientation in primate v1.
\newblock The Journal of Neuroscience 32: 10618--10626.
\bibAnnoteFile{Berens:2012}

\bibitem{Alivisatos:2013}
Alivisatos AP, Chun M, Church GM, Deisseroth K, Donoghue JP, et~al. (2013) The
  brain activity map.
\newblock Science 339: 1284--1285.
\bibAnnoteFile{Alivisatos:2013}

\bibitem{Anderson:2003}
Anderson T (2003) An introduction to multivariate statistical analysis.
\newblock Wiley series in probability and statistics .
\bibAnnoteFile{Anderson:2003}

\bibitem{Dempster:1972}
Dempster A (1972) Covariance selection.
\newblock Biometrics : 157--175.
\bibAnnoteFile{Dempster:1972}

\bibitem{Baba:2004}
Baba K, Shibata R, Sibuya M (2004) Partial correlation and conditional
  correlation as measures of conditional independence.
\newblock Australian \& New Zealand Journal of Statistics 46: 657--664.
\bibAnnoteFile{Baba:2004}

\bibitem{Chandrasekaran:2010}
Chandrasekaran V, Parrilo PA, Willsky AS (2010) Latent variable graphical model
  selection via convex optimization.
\newblock In: Communication, Control, and Computing (Allerton), 2010 48th
  Annual Allerton Conference on. IEEE, pp. 1610--1613.
\bibAnnoteFile{Chandrasekaran:2010}

\bibitem{Ma:2013}
Ma S, Xue L, Zou H (2013) Alternating direction methods for latent variable
  gaussian graphical model selection.
\newblock Neural computation : 1--27.
\bibAnnoteFile{Ma:2013}

\bibitem{Hertz:2011}
Hertz J, Roudi Y, Tyrcha J (2013) Ising models for inferring network structure
  from spike data.
\newblock In: Quiroga RQ, Panzeri S, editors, Principles of neural coding, CRC
  Press.
\bibAnnoteFile{Hertz:2011}

\bibitem{Jaynes:1957}
Jaynes ET (1957) Information theory and statistical mechanics.
\newblock Phys Rev 106: 620--630.
\bibAnnoteFile{Jaynes:1957}

\bibitem{Reddy:2005}
Reddy GD, Saggau P (2005) Fast three-dimensional laser scanning scheme using
  acousto-optic deflectors.
\newblock J Biomed Opt 10: 064038.
\bibAnnoteFile{Reddy:2005}

\bibitem{Katona:2012}
Katona G, Szalay G, Ma{\'a}k P, Kasz{\'a}s A, Veress M, et~al. (2012) Fast
  two-photon in vivo imaging with three-dimensional random-access scanning in
  large tissue volumes.
\newblock Nat Methods .
\bibAnnoteFile{Katona:2012}

\bibitem{Cotton:2013}
Cotton RJ, Froudarakis E, Storer P, Saggau P, Tolias AS (2013)
  Three-dimensional mapping of microcircuit correlation structure.
\newblock Frontiers in Neural Circuits 7: 151.
\bibAnnoteFile{Cotton:2013}

\bibitem{Vogelstein:2010}
Vogelstein JT, Packer AM, Machado TA, Sippy T, Babadi B, et~al. (2010) Fast
  nonnegative deconvolution for spike train inference from population calcium
  imaging.
\newblock Journal of neurophysiology 104: 3691--3704.
\bibAnnoteFile{Vogelstein:2010}

\bibitem{Feldt:2011}
Feldt S, Bonifazi P, Cossart R (2011) Dissecting functional connectivity of
  neuronal microcircuits: experimental and theoretical insights.
\newblock Trends in neurosciences 34: 225--236.
\bibAnnoteFile{Feldt:2011}

\bibitem{Malmersjo:2013}
Malmersj{\"o} S, Rebellato P, Smedler E, Planert H, Kanatani S, et~al. (2013)
  Neural progenitors organize in small-world networks to promote cell
  proliferation.
\newblock Proceedings of the National Academy of Sciences 110: E1524--E1532.
\bibAnnoteFile{Malmersjo:2013}

\bibitem{Sadovsky:2014}
Sadovsky AJ, MacLean JN (2014) Mouse visual neocortex supports multiple
  stereotyped patterns of microcircuit activity.
\newblock The Journal of Neuroscience 34: 7769-7777.
\bibAnnoteFile{Sadovsky:2014}

\bibitem{Alonso:1998}
Alonso JM, Martinez LM (1998) Functional connectivity between simple cells and
  complex cells in cat striate cortex.
\newblock Nature neuroscience 1: 395--403.
\bibAnnoteFile{Alonso:1998}

\bibitem{Aertsen:1989}
Aertsen A, Gerstein G, Habib M, Palm G (1989) Dynamics of neuronal firing
  correlation: modulation of ``effective connectivity".
\newblock Journal of neurophysiology 61: 900--917.
\bibAnnoteFile{Aertsen:1989}

\bibitem{Pillow:2008}
Pillow JW, Shlens J, Paninski L, Sher A, Litke AM, et~al. (2008)
  Spatio-temporal correlations and visual signalling in a complete neuronal
  population.
\newblock Nature 454: 995--999.
\bibAnnoteFile{Pillow:2008}

\bibitem{Schneidman:2006}
Schneidman E, Berry MJ 2nd, Segev R, Bialek W (2006) Weak pairwise correlations
  imply strongly correlated network states in a neural population.
\newblock Nature 440: 1007-12.
\bibAnnoteFile{Schneidman:2006}

\bibitem{Tkacik:2006}
Tkacik G, Schneidman E, Berry I, Michael J, Bialek W (2006) Ising models for
  networks of real neurons.
\newblock arXiv preprint q-bio/0611072 .
\bibAnnoteFile{Tkacik:2006}

\bibitem{Yu:2008}
Yu S, Huang D, Singer W, Nikolic D (2008) A small world of neuronal synchrony.
\newblock Cereb Cortex 18: 2891-901.
\bibAnnoteFile{Yu:2008}

\bibitem{Tang:2008}
Tang A, Jackson D, Hobbs J, Chen W, Smith JL, et~al. (2008) A maximum entropy
  model applied to spatial and temporal correlations from cortical networks in
  vitro.
\newblock J Neurosci 28: 505-18.
\bibAnnoteFile{Tang:2008}

\bibitem{Shlens:2009}
Shlens J, Field GD, Gauthier JL, Greschner M, Sher A, et~al. (2009) The
  structure of large-scale synchronized firing in primate retina.
\newblock J Neurosci 29: 5022-31.
\bibAnnoteFile{Shlens:2009}

\bibitem{Zalesky:2012}
Zalesky A, Fornito A, Bullmore E (2012) On the use of correlation as a measure
  of network connectivity.
\newblock Neuroimage 60: 2096--2106.
\bibAnnoteFile{Zalesky:2012}

\bibitem{Grewe:2010}
Grewe BF, Langer D, Kasper H, Kampa BM, Helmchen F (2010) High-speed in vivo
  calcium imaging reveals neuronal network activity with near-millisecond
  precision.
\newblock Nat Meth 7: 399--405.
\bibAnnoteFile{Grewe:2010}

\bibitem{Theis:2014}
Theis L, Berens P, Froudarakis E, Reimer J, Roman-Roson M, et~al. (2014)
  Supervised learning sets benchmark for robust spike detection from calcium
  imaging signals.
\newblock bioRxiv : 010777.
\bibAnnoteFile{Theis:2014}

\bibitem{Leung:2013}
Leung LC, Wang GX, Mourrain P (2013) Imaging zebrafish neural circuitry from
  whole brain to synapse.
\newblock Frontiers in neural circuits 7.
\bibAnnoteFile{Leung:2013}

\bibitem{Ahrens:2013}
Ahrens MB, Orger MB, Robson DN, Li JM, Keller PJ (2013) Whole-brain functional
  imaging at cellular resolution using light-sheet microscopy.
\newblock Nature methods 10: 413--420.
\bibAnnoteFile{Ahrens:2013}

\bibitem{Perin:2011}
Perin R, Berger TK, Markram H (2011) A synaptic organizing principle for
  cortical neuronal groups.
\newblock Proc Natl Acad Sci U S A 108: 5419-24.
\bibAnnoteFile{Perin:2011}

\bibitem{Gerstein:1989}
Gerstein GL, Bedenbaugh P, Aertsen AM (1989) Neuronal assemblies.
\newblock Biomedical Engineering, IEEE Transactions on 36: 4--14.
\bibAnnoteFile{Gerstein:1989}

\bibitem{Chapin:1999}
Chapin JK, Nicolelis MA (1999) Principal component analysis of neuronal
  ensemble activity reveals multidimensional somatosensory representations.
\newblock Journal of neuroscience methods 94: 121--140.
\bibAnnoteFile{Chapin:1999}

\bibitem{Peyrache:2010}
Peyrache A, Benchenane K, Khamassi M, Wiener SI, Battaglia FP (2010) Principal
  component analysis of ensemble recordings reveals cell assemblies at high
  temporal resolution.
\newblock Journal of computational neuroscience 29: 309--325.
\bibAnnoteFile{Peyrache:2010}

\bibitem{Ch:2010}
Ch'Ng YH, Reid RC (2010) Cellular imaging of visual cortex reveals the spatial
  and functional organization of spontaneous activity.
\newblock Frontiers in integrative neuroscience 4.
\bibAnnoteFile{Ch:2010}

\bibitem{Lopes:2011}
Lopes-dos Santos V, Conde-Ocazionez S, Nicolelis MA, Ribeiro ST, Tort AB (2011)
  Neuronal assembly detection and cell membership specification by principal
  component analysis.
\newblock PloS one 6: e20996.
\bibAnnoteFile{Lopes:2011}

\bibitem{Lopes:2013}
Lopes-dos Santos V, Ribeiro S, Tort AB (2013) Detecting cell assemblies in
  large neuronal populations.
\newblock Journal of Neuroscience Methods 220: 149 -- 166.
\bibAnnoteFile{Lopes:2013}

\bibitem{Okun:2012}
Okun M, Yger P, Marguet SL, Gerard-Mercier F, Benucci A, et~al. (2012)
  Population rate dynamics and multineuron firing patterns in sensory cortex.
\newblock J Neurosci 32: 17108-19.
\bibAnnoteFile{Okun:2012}

\bibitem{Ganmor:2011}
Ganmor E, Segev R, Schneidman E (2011) Sparse low-order interaction network
  underlies a highly correlated and learnable neural population code.
\newblock Proc Natl Acad Sci U S A 108: 9679-84.
\bibAnnoteFile{Ganmor:2011}

\bibitem{Tkacik:2013}
Tka{\v{c}}ik G, Marre O, Amodei D, Schneidman E, Bialek W, et~al. (2013)
  Searching for collective behavior in a network of real neurons.
\newblock arXiv preprint arXiv:13063061 .
\bibAnnoteFile{Tkacik:2013}

\bibitem{Pfau:2013}
Pfau D, Pnevmatikakis EA, Paninski L (2013) Robust learning of low-dimensional
  dynamics from large neural ensembles.
\newblock In: Advances in Neural Information Processing Systems. pp.
  2391--2399.
\bibAnnoteFile{Pfau:2013}

\bibitem{Koster:2013}
K{\"o}ster U, Sohl-Dickstein J, Gray CM, Olshausen BA (2013) Higher order
  correlations within cortical layers dominate functional connectivity in
  microcolumns.
\newblock arXiv preprint arXiv:13010050 .
\bibAnnoteFile{Koster:2013}

\bibitem{Roudi:2009}
Roudi Y, Nirenberg S, Latham PE (2009) Pairwise maximum entropy models for
  studying large biological systems: when they can work and when they can't.
\newblock PLoS computational biology 5: e1000380.
\bibAnnoteFile{Roudi:2009}

\bibitem{Arlot:2010}
Arlot S, Celisse A (2010) A survey of cross-validation procedures for model
  selection.
\newblock Statistics Surveys 4: 40--79.
\bibAnnoteFile{Arlot:2010}

\bibitem{Efron:1977}
Efron B, Morris CN (1977) Stein's paradox in statistics.
\newblock WH Freeman.
\bibAnnoteFile{Efron:1977}

\bibitem{Song:2005}
Song S, Sj{\"o}str{\"o}m PJ, Reigl M, Nelson S, Chklovskii DB (2005) Highly
  nonrandom features of synaptic connectivity in local cortical circuits.
\newblock PLoS biology 3: e68.
\bibAnnoteFile{Song:2005}

\bibitem{Oswald:2008}
Oswald AMM, Reyes AD (2008) Maturation of intrinsic and synaptic properties of
  layer 2/3 pyramidal neurons in mouse auditory cortex.
\newblock Journal of neurophysiology 99: 2998.
\bibAnnoteFile{Oswald:2008}

\bibitem{Adesnik:2010}
Adesnik H, Scanziani M (2010) Lateral competition for cortical space by
  layer-specific horizontal circuits.
\newblock Nature 464: 1155--1160.
\bibAnnoteFile{Adesnik:2010}

\bibitem{Fino:2011}
Fino E, Yuste R (2011) Dense inhibitory connectivity in neocortex.
\newblock Neuron 69: 1188-203.
\bibAnnoteFile{Fino:2011}

\bibitem{Isaacson:2011}
Isaacson JS, Scanziani M (2011) How inhibition shapes cortical activity.
\newblock Neuron 72: 231--243.
\bibAnnoteFile{Isaacson:2011}

\bibitem{Levy:2012}
Levy RB, Reyes AD (2012) Spatial profile of excitatory and inhibitory synaptic
  connectivity in mouse primary auditory cortex.
\newblock J Neurosci 32: 5609-19.
\bibAnnoteFile{Levy:2012}

\bibitem{Packer:2011}
Packer AM, Yuste R (2011) Dense, unspecific connectivity of neocortical
  parvalbumin-positive interneurons: a canonical microcircuit for inhibition?
\newblock The Journal of Neuroscience 31: 13260--13271.
\bibAnnoteFile{Packer:2011}

\bibitem{Reimer:2014}
Reimer J, Froudarakis E, Cadwell CR, Yatsenko D, Denfield GH, et~al. (2014)
  Pupil fluctuations track fast switching of cortical states during quiet
  wakefulness.
\newblock Neuron 84: 355--362.
\bibAnnoteFile{Reimer:2014}

\bibitem{Fu:2014}
Fu Y, Tucciarone JM, Espinosa JS, Sheng N, Darcy DP, et~al. (2014) A cortical
  circuit for gain control by behavioral state.
\newblock Cell 156: 1139--1152.
\bibAnnoteFile{Fu:2014}

\bibitem{Meinshausen:2006}
Meinshausen N, B{\"u}hlmann P (2006) High-dimensional graphs and variable
  selection with the lasso.
\newblock The Annals of Statistics 34: 1436--1462.
\bibAnnoteFile{Meinshausen:2006}

\bibitem{Friedman:2008}
Friedman J, Hastie T, Tibshirani R (2008) Sparse inverse covariance estimation
  with the graphical lasso.
\newblock Biostatistics 9: 432--441.
\bibAnnoteFile{Friedman:2008}

\bibitem{Fazel:2002}
Fazel M (2002) Matrix rank minimization with applications.
\newblock Ph.D. thesis, Stanford University.
\bibAnnoteFile{Fazel:2002}

\bibitem{Recht:2010}
Recht B, Fazel M, Parrilo PA (2010) Guaranteed minimum-rank solutions of linear
  matrix equations via nuclear norm minimization.
\newblock SIAM review 52: 471--501.
\bibAnnoteFile{Recht:2010}

\bibitem{Vogels:1989}
Vogels R, Spileers W, Orban G (1989) The response variability of striate
  cortical neurons in the behaving monkey.
\newblock Experimental brain research 77: 432--436.
\bibAnnoteFile{Vogels:1989}

\bibitem{Ponce:2013}
Ponce-Alvarez A, Thiele A, Albright TD, Stoner GR, Deco G (2013)
  Stimulus-dependent variability and noise correlations in cortical mt neurons.
\newblock Proceedings of the National Academy of Sciences 110: 13162--13167.
\bibAnnoteFile{Ponce:2013}

\end{thebibliography}

\newpage
\section*{Figure Legends}
% This section is for figure legends only, do not include
% graphics in your manuscript file.

\begin{figure}[h!]
\caption{
{\bf Regularized estimators whose structure matches the true structure in the data are more efficient.}
     {\bf Row 1.} Graphical models of the target estimates of the four respective regularized covariance matrix estimators.  Recorded neurons are represented by the green spheres and latent units by the lightly shaded spheres.  Edges represent conditional dependencies, \emph{i.e.}\;`interactions'.
     {\bf Row 1, A}.  For estimator $C_{\sf diag}$, the target estimate is a diagonal matrix, which describes systems that lack linear dependencies.
     {\bf  Row 1, B.} For estimator $C_{\sf factor}$, the target estimate is a factor model (low-rank matrix plus a diagonal matrix), representing systems in which correlations arise due to common input from latent units.
     {\bf  Row 1, C}. For estimator $C_{\sf sparse}$, the covariance matrix is approximated as the inverse of a sparse matrix. This approximation describes systems in which correlations arise from a sparse set of  linear associations between the observed units.
     {\bf  Row 1, D}.  For estimator $C_{\sf sparse+latent}$, the covariance matrix is approximated as the inverse of the sum of a sparse matrix and a low-rank matrix. This approximation describes a model wherein correlations arise due to sparse associations between the recorded cells \emph{and} due to several latent units.
     \\
     {\bf Row 2:} Examples of $50\times 50$ correlation matrices corresponding to each structure: {\bf A.} the diagonal correlation matrix, {\bf B.} a factor model with four latent units, {\bf C.}  a correlation matrix with 67\%  off-diagonal zeros in its inverse, and {\bf  D.} a correlation matrix whose inverse is the sum of a rank-3 matrix (\emph{i.e.}\;three latent units) and a sparse matrix with 76\% off-diagonal zeros.
     \\
{\bf Row 3:} Sample correlation matrices calculated from samples of size $n=500$ drawn from simulated random processes with respective correlation matrices shown in Row 2.  The structure of the sample correlation matrix is difficult to discern by eye.
     \\
{\bf Row 4:} Estimates computed from the same data as in Row 3 using structured estimators of the correct type, optimized by cross-validation.  The regularized estimates are closer to the truth than the sample correlation matrices.
     \\
{\bf Row 5:} True loss (Eq.~\ref{eq:loss}) for the five estimators as a function of sample size. The error bars indicate the standard deviation of the mean.  Estimators with structure that matches the true model converged to zero faster than the other estimators.
     \\
{\bf Row 6:} Validation loss (Eq.~\ref{eq:vloss}) for the five estimators relative to the matching estimators for each type of ground truth. Error bars indicate the standard deviation of the mean.  Differences in validation loss approximate differences in true loss.
}
\label{fig:1}
\end{figure}

\begin{figure} 
\caption{
{\bf Performance of covariance estimators on samples drawn from Ising models.} 
{\bf A--D} Validation losses of covariance matrix estimators relative to the estimator whose structure matches the ground truth. The calculation is performed identically to Fig.~\ref{fig:1} Row 6 except Ising models are used as ground truth. 
}
\label{fig:2}
\end{figure}

\begin{figure}
\caption{{\bf Acquisition of neural signals for the estimation of noise correlations.}
Visual stimuli comprising full-field drifting gratings interleaved with blank screens ({\bf A}) presented during two-photon recordings of somatic calcium signals using fast 3D random-access microscopy ({\bf B}).
{\bf C--F.} Calcium activity data from an example site.
{\bf C.} Representative calcium signals of seven cells, downsampled to 20 Hz, out of the 292 total recorded cells. Spiking activity inferred by nonnegative deconvolution is shown by red ticks below the trace.
{\bf D.} The spatial arrangement and orientation tuning of the 292 cells from the imaged site. The cells' colors indicate their orientation preferences. The gray cells were not significantly tuned.
{\bf E.} The sample noise correlation matrix of the activity of the neural population.
{\bf F.} Histogram of noise correlation coefficients in one site. The red line indicates the mean correlation coefficient of 0.020.
} \label{fig:3}
\end{figure}

\begin{figure}
\caption{
{\bf Performance of estimator $C_{\sf sparse+latent}$ expressed as validation loss (eq.~\ref{eq:vloss}) relative to the other estimators: $C_{\sf sample}$, $C_{\sf diag}$, $C_{\sf factor}$, and $C_{\sf sparse}$.}
Covariance estimators $C_{\sf sample}$, $C_{\sf diag}$, $C_{\sf factor}$, and $C_{\sf sparse}$ produced consistently greater validation losses than $C_{\sf sparse+latent}$ ($p<0.01$ in each comparison, Wilcoxon signed rank test, $n=27$ sites in 14 mice). The box plots indicate the $25^{th}$, $50^{th}$, and $75^{th}$ percentiles with the whiskers extending to the minimum and maximum values after excluding the outliers marked with `+'.
} \label{fig:4}
\end{figure}

\begin{figure}
\caption{{\bf Structure revealed by $C_{\sf sparse+latent}$.}
{\bf A, B.} The regularized estimate $C_{\sf sparse+latent}$ closely approximates the sample correlation matrix $C_{\sf sample}$.
{\bf C, D.} The partial correlation matrices from the two estimates differ substantially.
{\bf E.} The partial correlation matrix of the regularized estimate is decomposed into a sparse component with 92.8\% off-diagonal zeros (bottom-left) and low-rank component of rank 72 (top-right).
{\bf F.} The sparse component of the regularized partial correlation matrix had little resemblance to the sample correlations: The gray region indicates the range of correlations containing 92.8\% of cells pairs, equal to the fraction of zeros in the sparse partial correlation matrix. Correlation coefficients outside this interval formed the network of greatest correlations.  This network differed from the sparse component of the $C_{\sf sparse+latent}$:  Only 27.7\% of the highest correlations coefficients outside the gray regions coincided with interactions inferred by $C_{\sf sparse+latent}$.
{\bf G.} A graphical depiction of the positive (green) and negative (magenta) sparse partial correlations as edges between observed neurons. The line density is proportional to the magnitude of the partial correlation.
{\bf H.} A subset of neurons from the center of the cluster shown in {\bf G} showing the sparse partial correlations.
{\bf I.} The same subset of neurons with edges indicating sample correlations thresholded to match the sparsity of the sparse partial correlation. These edges correspond to the sample correlation coefficients outside the gray region in panel F.
}
\label{fig:5}
\end{figure}
 
\begin{figure}
\caption{{\bf Properties of $C_{\sf sparse+latent}$ estimates from all imaged sites.}
Each point represents an imaged site with its color indicating the population size as shown in panels A and B. The example site from Figures \ref{fig:3} and \ref{fig:5} is circled in blue.
\\
{\bf A.} The number of inferred latent units \emph{vs.}~population size.
{\bf B.} The connectivity of the sparse component of partial correlations as a function of population size.
{\bf C.} The average sample correlations \emph{vs.}~the average partial correlations (Eq.~\ref{eq:partial}) of the $C_{\sf sparse+latent}$ estimate.
{\bf D.} The percentage of negative interactions vs.~connectivity in the $C_{\sf sparse+latent}$ estimates.
}
\label{fig:6}
\end{figure}


\begin{figure}
\caption{
{\bf Dependence of sample correlations, regularized partial correlations, and connectivity inferred by $C_{\sf sparse+latent}$ on the differences in preferred orientations, $\Delta \mbox{ori}$, and physical distances: horizontal $\Delta x$ and depth $\Delta z$.} 
Five sites with highest connectivity (see Fig.~\ref{fig:6} B) were selected for this analysis.
\\
{\bf A--C.} Mean sample correlations in relation to $\Delta\mbox{ori}$,  $\Delta x$ and $\Delta z$, respectively. For $\Delta x$ averages, only horizontally aligned cell pairs with $\Delta z<30\,\mu m$ were considered. Similarly, for $\Delta z$ averages, only vertically aligned cell pairs with $\Delta x<30\,\mu m$ were considered.
\\
{\bf D--F.} Mean partial correlations regularized by the $C_{\sf sparse+latent}$ estimator binned the same way as the sample correlations above. The partial correlations exhibit stronger dependence on $\Delta\mbox{ori}$, $\Delta x$, and $\Delta z$ than sample correlations. 
\\
{\bf G--I.} Positive connectivity (green) and negative connectivity (red) inferred by the $C_{\sf sparse+latent}$ estimator. 
Positive and negative connectivities refer to the fractions of the positive and negative partial correlations computed from the sparse component $S$ of $C_{\sf sparse+latent}$.  
Positive connectivity decreases with $\Delta \mbox{ori}$, $\Delta x$, and $\Delta z$. 
Negative connectivity does not decrease with $\Delta \mbox{ori}$, $\Delta x$ within the examined range, and with $\Delta z$ for small values of $\Delta z<60\,\mu m$.
}
\label{fig:7}
\end{figure}

%\section*{Tables}
% 
% See introductory notes if you wish to include sideways tables.
%
% NOTE: Please look over our table guidelines at http://www.plosone.org/static/figureGuidelines#tables to make sure that your tables meet our requirements. Certain types of spacing, cell merging, and other formatting tricks may have unintended results and will be returned for revision.
%
%\begin{table}[!ht]
%\caption{
%\bf{Table title}}
%\begin{tabular}{|c|c|c|}
%table information
%\end{tabular}
%\begin{flushleft}Table caption
%\end{flushleft}
%\label{tab:label}
% \end{table}

\newpage
\section*{Supporting Information Legends}
%
% Please enter your Supporting Information captions below in the following format:
%\item{\bf Figure SX. Enter mandatory title here.} Enter optional descriptive information here.
% 
\begin{description}
\item {\bf Figure S1. Optimization of hyperparameters of the $C_{\sf sparse+latent}$ estimator.} {\bf A.} Validation loss (Eq.~\ref{eq:full-loss}) for the example site in Fig.~\ref{fig:3} and \ref{fig:5} as a function of the hyperparameters $\alpha$ and $\beta$ of the $C_{\sf sparse+latent}$ estimator (Eq.~\ref{eq:c-sl} and Eq.~\ref{eq:ma}). In all panels, the red cross marks the optimal value found by the pattern search algorithm described in Methods.
{\bf B.} The connectivity ($1-\mbox{sparsity}$) of the sparse component $S$ as a function of $\alpha$ and $\beta$ for the example site.
{\bf C.} The number of latent units, \emph{i.e.}~the rank of the low-rank component $L$, as a function of hyperparameters $\alpha$ and $\beta$.
{\bf D.} The loss function as a function of the connectivity and the number of latent units.
\end{description}
\end{document}

