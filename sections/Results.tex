
\subsection*{Covariance estimation}
We aim to estimate the true covariance matrix 
\begin{equation}
\Sigma = \E{(x-\mu)(x-\mu)^\T}
\end{equation}
where $\E{\cdot}$ denotes expectation  and $x$ is the $p\times 1$ vector of real-valued instantaneous firing rates discretized into bins of duration $\Delta t$ and $\mu = \E{x}$.  

The usual estimator of the covariance matrix is the \emph{sample covariance matrix} computed from the empirical sample  $x(1),\ldots,x(n)$:
\begin{equation}
\hat \Sigma_0 = \frac 1 \nu \sum\limits_{t=1}^n (x(t)-\mu)(x(t)-\mu)^\T, 
\end{equation}
where $x(t)$ are sequential observations of population activity and $\nu$ is the number of degrees of freedom ($\nu=n-1$ if observations are independent).

The sample covariance matrix is unbiased i.e.\;$\E{\hat\Sigma_0}-\Sigma=0$.  For finite sample sizes, however, $\hat\Sigma_0$ is not as close to $\Sigma$ as a number of biased estimators that rely on \emph{regularization}. Regularization is the deliberate biasing of the estimate toward a low-dimensional, less variable \emph{target estimate} to strike a favorable balance between bias and variability of the estimate \cite{Bickel:2006,Ledoit:2004}.  The estimator whose target estimate most closely matches the true low-dimensional structure of the data is likely to outperform other estimators.

We considered four regularized estimators based on distinct families of low-dimensional target estimates: independent, latent factors, sparse partial correlations, and sparse partial correlations with latent factors. These target estimates are depicted graphically in Fig.~\ref{fig:02}.  

\input{sections/Figure2.tex}

The relative performance of the four estimators on synthetic and empirical data was evaluated by cross-validated multivariate normal log-likelihood (see Methods for detailed justification).  
Each estimator has one or two \emph{hyperparameters}, which are estimated within training datasets by nested cross-validation (See Methods). 

In the first regularized estimator, the target estimate is a diagonal matrix $\hat D$.  The optimized estimate $\hat\Sigma_{\rm diag}$ is obtained by the linear \emph{shrinkage} of the unbiased estimate $\hat\Sigma_0$ toward $\hat D$ as regulated by scalar \emph{shrinkage intensity} $\lambda \in [0, 1]$:
\begin{equation}
\hat \Sigma_{\rm diag} = (1-\lambda) \hat \Sigma_0 + \lambda \hat D
\end{equation}
The diagonal target estimate expresses the idea of lack of dependence (or of linear association) between the activity of observed neurons (Fig.~\ref{fig:02}A).  If this assumption aptly describes recorded data, then strong shrinkage toward $\hat D$ will add little bias while strongly reducing the variability of the estimate. 

In the second regularized estimator, the target estimate is the factor model $\hat F = LL^\T + \Psi$ with $d$ factors so that $L$ is the $p\times d$ matrix of \emph{factor loadings} and the diagonal matrix $\Psi$ contains the indvidual variances of each neuron.
Then the estimate is 
\begin{equation}
\hat \Sigma_{\rm factor} = (1-\lambda) \hat \Sigma_0 + \lambda \hat F,
\end{equation}
This estiamtor has two hyperparameters: the number of factors $d$ and shrinkage intensity $\lambda$. The target estimate $\hat F$ expresses the idea that the fluctuations in population activity must be driven by relatively small number of unobserved sources that affect many cells while direct interactions between cells are insignificant (Fig.~\ref{fig:02}B).   

The third estimator is based on sparsifying the inverse covariance (precision) matrix. It expresses the idea that, similar to synaptic connections, interactions between neurons are sparse (Fig.~1C). The estimator produces a low-dimensional approximation to the covariance that has many zeros in its inverse. This approximation problem is also known as covariance selection. The covariance matrix estimate is given by
\begin{equation}
\hat\Sigma = \hat S^{-1}
\quad\mbox{with}\quad
\hat S = \argmin\limits_{\|S\|_0 \le \rho} \loss{S^{-1},\hat\Sigma_0}   
\end{equation}
where $\mathcal L$ is a loss function (see below) and $\|S\|_0\le\rho$ signifies the constraint that $S$ has at most $\rho$ non-zero coefficients. While solving the problem in this form is computationally challenging, relaxing the $L_0$ norm to the $L_1$ norm converts it into one of convex optimization \cite{Donoho:2000}, which is computationally much more tractable. The resulting algorithm, which is known as graphical lasso \cite{Meinshausen:2006,Yuan:2007,Banerjee:2008,Friedman:2008}, produces the following estimator  
\begin{equation}
\hat\Sigma_{\rm sparse}^\mathcal{C} = \hat S^{-1}
\quad\mbox{with}\quad
\hat S = \argmin \loss{S^{-1},\hat\Sigma_0} + \lambda \|S\|_1,
\end{equation}
where $\|S\|_1$ is the $L_1$ norm of the precision matrix (see Methods for details) and $\lambda$ is again determined by cross-validation.

Finally, we consider a fourth estimator, which combines common inputs with sparse interactions (Fig.~1D). For this estimator the covariance is
\begin{equation}
\hat\Sigma_{\rm combined} = (S + LL^\T)^{-1},
\end{equation}
where, as above, $S$ is sparsified using $L_1$ norm and $L$ is a low-rank matrix with the rank determined by cross-validation.



\subsection*{Simulation using toy data}

To verify our approach and to illustrate the performance of our four regularized estimators and the sample covariance estimator, we constructed five model populations with different underlying structures. Each population contained 100~neurons. The first four populations matched the low-dimensional structure of our four estimators: independent (Fig.~2\,A), low-rank (Fig.~2\,B), sparse inverse (Fig.~2\,C) and low-rank combined with sparse inverse (Fig.~2\,D). \Acomment{Integrate schematics from Fig.~1 into this figure (make it the first row) for better correspondence} In addition, we considered a fifth population that had no low-dimensional structure (Fig.~2\,E).

To evaluate the performance of the different estimators, we computed the excess loss for all combinations of model populations and estimators, including the sample covariance. The first striking observation is that in all cases all four regularized estimators performed substantially better than the sample covariance (Fig.~2, fourth row). In particular, this is even true for the case where the population did not have any low-dimensional structure at all (Fig.~2\,E). While it may appear surprising at first sight, this counter-intuitive phenomenon is known as \emph{Stein's phenomenon} or \emph{Stein's paradox} \cite{Efron:1977}, named after its discoverer Charles Stein \cite{Stein:1956}. A common misconception about regularization is that its effect depends on accurate prior knowledge about the structure of the data. However, substantial improvement can be attained by shrinking the unbiased estimate toward an arbitrary target as long as the target is less variable than the unbiased estimator. The more accurate description of regularization is as of the optimal tradeoff between estimation and approximation error -- the so-called ``bias-variance tradeoff''.

Thus, if taken in isolation a regularized estimator improves the estimate we should not interpret this result to suggest that the estimator's target has the same low-dimensional structure as the data-generating process. However, when comparing multiple estimators against each other, the one whose target estimate most closely matches the true value with the smallest number of parameters will reduce the estimation error with the least increase in approximation error. Indeed, in all four toy examples with a low-dimensional structure, the estimator with the matching regularization target performed best (Fig.~2\,A--D, fourth row). Since the estimator combining low-rank and sparse interactions combines to low-dimensional targets and includes the simpler estimators as special cases, it performed almost as well even when the low-dimensional structure did not include either a low-rank component or sparse interactions (Fig.~2\,B,\,C). \Acomment{May want to include a smaller sample size as well since here the more reduced estimators (B, C) may actually outperform estimator D when the population structure matches their target and there is little data (say, 1.5x number of neurons)} 

Since the above evaluations of excess loss require knowledge of ground truth, this analysis can be done only on toy data. However, an unbiased empirical estimate of the excess loss exists (see Methods, Eq.~\ref{eq:validationLoss} \Acomment{This may need a brief sentence about the intuition what's the difference, or a reference to the relevant methods section}).  To verify that our approach should also work in a realistic situation without access to ground truth, we also computed the empirical loss. Indeed, without access to ground truth, this analysis revealed a pattern of results that reproduced the results obtained with access to ground truth above (Fig.~2, last row). Because validation loss is computed by comparing estimates to noisy sample covariance matrices from smaller validation sets, empirical loss is a much noisier measurement than excess loss. In addition, it does not converge to zero with increasing sample size as excess loss does.




\subsection*{Covariance estimation on neural data}


We recorded the calcium activity of dense populations of neurons in the supragranular layers in primary visual cortex of anesthetized mice using fast random-access 3D scanning two-photon microscopy \cite{Stosiek:2003,Reddy:2005}. We presented numerous repetitions of full-field drifting gratings (Fig. 1A and 1B) to the eye contralateral to the imaged site. This technique allowed us to record from a large number (150--350) of cells in a small volume of cortical tissue ($200\times200\times100$ $\mu$m$^3$) in layers 2/3 and 4. We deconvolved somatic calcium signals using sparse nonnegative deconvolution \cite{Vogelstein:2010} (Fig.\;1C and 1D) and subtracted the average stimulus response to remove effects of the stimulus. From this residual response we computed the sample noise covariance matrix (Fig.\;1E).

In such highly localized populations both direct interactions between cells and common diffuse inputs are likely to contribute to the overall population variability. At the same time, most correlations are relatively small (Fig.~1\,E), suggesting that a simple shrinkage towards independence may provide a sufficiently well regularized estimate. By comparing the performance of our differently regularized estimators we can gain insights into which of these aspects are important in our data.

As expected, all regularized estimators outperformed the sample covariance estimator substantially (Fig.~4) \Acomment{I think we should show that}. Say something about how shrinkage, low-rank and sparse inverse relate \Acomment{Why do we do pairwise comparisons instead of just showing median (across sites) log loss relative to the combined estimator? I've seen many people do this and it provides an ordering. I feel like we should come up with a way of ordering them somehow, even if it's under certain assumptions that may not be entirely correct. I'm pretty sure if we don't do it the reviewers will bring it up anyway...}. Finally, the combined sparse and low-rank estimator dominated all others significantly (Fig.~4), showing that both hidden units and direct interactions are important in our data. The improvement from the low-rank estimator to the combined one is much larger than that from the sparse inverse to the combined one, suggesting that in this dataset direct interactions contribute more strongly to the correlation structure than hidden units do.



\subsection*{Relationship between functional covariance structure and circuit architecture}


\begin{itemize}
\item Linear and partial correlations versus spatial separation (lateral and vertical). Discuss whether to include or remove thresholded correlations.
\item Magnitude of common input versus spatial location in the volume (lateral and vertical separation from center).
\item Linear and partial correlations versus orientation preference
\item Distribution of sparsity over sites
\item Distribution of number of hidden units over sites
\item Distribution of eigenvalues for sample covariance versus combined estimator
\item etc. etc. 

\end{itemize}





