\hl{\tiny why correlations:}
Linear correlations between the spiking activity of pairs of neurons, or simply \emph{neural correlations}, are among the most widely used and familiar descriptive statistics of neural population activity \citep{Cohen:2011}.  \Kcomment{Need some references to the original literature here.  The earliest I know are papers by Perkel, et al (1967).  The first paper I know where they have recorded from multiple neurons simultaneously is Clark and Gerstein,  Simultaneous studies
of firing patterns in several neurons, Science (1964).  This may be something to cite when you talk about the actual problem -- although in this paper they did not use correlation as a measure of dependence.  There is also a 1964 
RAND Corporation report entitled ``Detection of Functional Interactions Among Neurons: A technique using repetitive presentations of stimuli'' from 1964.  I've only seen it cited, and have no idea how to get it.}
For example, \emph{noise correlations}, i.e.~the correlations of stimulus response variability between pairs of neurons in sensory areas, have been of particular interest thanks, in part, to their profound theoretical implications for stimulus coding \citep{Zohary:1994,Abbott:1999,Averbeck:2006,Berens:2011}.  
Lending further credence to their role as indicators of detailed and specific functional connectivity \Kcomment{What is functional connectivity?  Isn't it essentially what we uncover using a method like estimating linear correlations?  I think we should be explicit about this.  The previous phrasing implies that there is some concrete ``functional connectivity'' that we are trying to uncover.} have been a series of discoveries of nontrivial relationships between neural correlations and other aspects of circuit organization such as the physical distance separating the neurons, their synaptic connectivity and stimulus tuning similarity, cortical layer specificity, cell-type specificity, progressive changes in development and in learning, changes due to sensory stimulation and global brain states, and others \citep{Kohn:2005,Smith:2008,Kohn:2009,Goard:2009,Golshani:2009,Renart:2010,Ecker:2010,Smith:2013,Denman:2013}.  \Kcomment{The previous sentence is too long. I think it contains two main ideas:  one is relation between functional connectivity and 
correlations, second, correlations and circuit physiology.  They are related, but could be separated. Also, pair discovery
with appropriate reference.}

\hl{\tiny ambiguity of interpretation:}
Neural correlations do not come with ready or unambiguous mechanistic interpretations.   
Theoretical and simulation studies show that neural correlations at various temporal scales arise from multiple underlying mechanisms such as direct synaptic interactions,  common or correlated inputs, shared sensory noise, chains of multiple synaptic connections, oscillations, top-down modulation, and background network activity \citep{Perkel:1967b,Shadlen:1998,Salinas:2001,Ostojic:2009} \Kcomment{also add ref to Rosenbaum and Josi\'{c} ``Mechanisms that modulate the transfer of spiking correlations. I think it is relevant.}

\hl{\tiny opportunities with multineuronal data:}
The neural correlation matrix obtained from multineuronal recordings is greater than the sum of its parts: it can be transformed into other representations  that accentuate different aspects of population activity.  
For example, the eigenvalue decomposition of the covariance matrix, also known as principal component analysis (PCA), expresses shared activity components across the entire population;  common fluctuations in the population activity of the entire circuit may be accurately represented by just a few principal components but may affect all correlation coefficients. 
In contrast, the inverse of the correlation matrix reveals the \emph{partial correlations} between neuron pairs.  Such partial correlations reflect specific pairwise linear dependence, conditioned on the activity of all other recorded cells. 
\Kcomment{It may be overkill, but I would add another sentence here to re-emphasize this point.  It is one that
many will probably miss, that is, they will not get the importance of the fact that this is \emph{conditioned} on the 
activity of other cells.}
A strong interaction between a pair of neurons may be expressly represented by a single partial correlation, 
but the effect of such interaction can propagate and affect other pairwise correlations, as well as principal components.


\hl{\tiny motivate dimensionality reduction:}
 Covariance estimates from real high-dimensional data  are both noisy and redundant.  The amount of recorded data increases only linearly with population size whereas the number of free parameters in the covariance matrix increases quadratically.  As a result, empirical covariance estimates for multidimensional data  accumulate substantial redundancy and noise.  \Kcomment{The last sentence in this paragraph says the same thing as the first sentence.  You may want to change this.}

To reduce the estimation error, statistically efficient estimators are regularized.  One regularization approach is to shrink the \Kcomment{Which estimate?  The point estimate?}  estimate toward a less variable low-dimensional estimate \citep{Schafer:2005}.  Another regularization approach, often called \emph{sparsification}, reduces the dimensionality of the estimate by setting its least significant parameters to zero. 
Regularized estimates perform favorably on new samples of data compared to the full model, provided the amount of regularization is chosen carefully. 

\hl{\tiny choosing between regularization schemes}
Estimator improvements due to regularization depend profoundly on the parameterization to which it is applied.  
\Kcomment{I would argue that more than that, it depends on the structure of the underlying data. In fact, I don't really
understand the previous statement.}
Sparsification \Kcomment{Will it be clear to this audience that sparsification is the same as setting some terms (eigenvalues, entries in a matrix) equal to zero?} of the eigenspectrum produces a low-rank estimate of the covariance matrix also known as truncated PCA \citep{Rothman:2008} \Kcomment{This is grammatically incorrect. The estimate cannot be known as the PCA, \emph{i.e.} the estimate is not an analysis.} or, allowing for additional independent variances, as factor analysis (FA) \citep{Fan:2008}. \Kcomment{This last part of the sentence is obscure.  What ``additional variances''?}  Sparsification of the inverse covariance is known as covariance selection \citep{Dempster:1972,Friedman:2008} and is related to finding a graphical model in which a large fraction of neuron pairs are conditionally independent of each other.  Representations that are capable of concentrating the significant real effects in a small number of parameters will reap greatest benefits from sparsification over ones that distribute the real effects widely across many parameters.  Random noise tends to be broadly distributed across all parameters in any parameterization. \Kcomment{Underlying all this is the idea of Occam's Razor, and I think you should at least mention
this. You do not need to talk about this from a Bayesian viewpoint, but you can say that you are avoiding overfitting
by choosing a smaller set of parameters.}


\hl{\tiny our approach and summar of findings} 
In this study, we compared the performance of several covariance estimation schemes for spatially compact \Kcomment{This is important, and you should explain a bit more about what you mean by spatially compact.  This could
mean different things.} groups of 150--300 neurons in layers 2-4 in mouse primary visual cortex during visual stimulation.   The performance of the estimators was evaluated by computing the mean squared error between the optimized covariance estimate fitted to training data and the non-regularized covariance matrix from a separate test data set.  Low-rank covariance estimates performed significantly better than shrinkage estimators, but estimators with sparse partial correlations were more efficient still. Typically, between 3 and 16\% of neuronal paris were connected by non-zero partial interactions.  Mixed sparse estimators with sparse partial correlations and low-rank components performed comparably. 
\Kcomment{Will you mention relation to physiology in the Discussion?}


As noted above, the optimal covariance estimator is domain-dependent. 
These specific findings may not spread to all neural circuits. \Kcomment{Do you mean that they may not hold 
for circuits in different areas of cortex?}
Improved accuracy estimation accuracy is not the main aim or the only benefit.  \Kcomment{Then what is -- I do not 
understand how you can have what you describe in what follows without an accurate estimate of partial correlations. 
Indeed, wouldn't an increase in accuracy help here, with the caveats that we discussed many times?}
The selection of the optimal regularization scheme in itself serves as an effective descriptor of the dominant correlation structures. The finding that a identifiable subsets of partial correlations were important to describe the partial correlation makes such pairs as prime candidates as direct functional interactions. 

\Kcomment{Several things are not discussed here -- I think it is important that we choose models, and 
estimation approaches according to what we think is true about the physiology.  The approaches
that you suggest above do have this connection -- FA because coherent drive may be the most important,
sparsifying the inverse,  because connectivity is sparse.   Thus there is a good reason to choose these
approaches, and it is not only because this is what statisticians give us. Also, what are the
shortcomings of linear correlations, \emph{i.e.} how can nonlinear interactions  change the interpretation.?} 