\subsection*{Evaluation of covariance matrix estimators}
The quality of a covariance matrix estimate $\hat\Sigma$ is measured by a real-valued \emph{loss function} $\loss{\hat\Sigma,\Sigma}$.  The loss function quantifies the error of the estimate, \emph{i.e}.~the deviation of $\hat\Sigma$ from $\Sigma$. The loss function must attain its minimum  when $\hat\Sigma = \Sigma$.  

For the purposes of this study, we adopted the \emph{negative normal log-likelihood loss} function:
\begin{equation}\label{eq:loss}
\loss{\hat\Sigma,\Sigma} = \frac 1 p\left[\ln \det \hat \Sigma + \Tr(\hat \Sigma^{-1}\Sigma)\right]
\end{equation}
This choice is motivated by mathematical convenience. Other popular choices for the loss function are the Frobenius norm of the difference $\hat\Sigma-\Sigma$ \cite{Ledoit:2004,Schafer:2005}, Stein's entropy loss, and quadratic loss \cite{James:1961,Fan:2008}.  We expect that the main conclusions of our study will not change qualitatively under other well behaved loss functions.

The aim of our project is to identify which covariance matrix estimator minimizes the expected loss 
\begin{equation}
r = \E{\loss{\hat\Sigma, \Sigma}}
\end{equation}
which is known as the \emph{risk} of $\hat\Sigma$.

In practice, the true value $\Sigma$ is not accessible and an estimators' risks must be estimated from the data.  This may be accomplished through \emph{validation}.  \Kcomment{If you will discuss validation later, point to it.}
Let $\hat\Sigma_0^\prime$ denote a sample covariance matrix measured from an independent sample that was not used included in the computation of $\hat\Sigma$. 

$\loss{\cdot,\cdot}$ is additive in its second argument so that \Kcomment{You frequently use ``such that'' instead of ``so that''}
 \begin{equation}\label{eq:additivity}
 \loss{\hat\Sigma,X_1} + \loss{\hat\Sigma,X_2} \equiv \loss{\hat\Sigma,X_1+X_2}
 \end{equation}
Then \emph{validation loss}  \TODO{check appropriate term}
\begin{equation}\label{eq:validationLoss}
\hat \ell = \loss{\hat\Sigma,\hat\Sigma_0^\prime}
\end{equation}
is an unbiased estimate of the risk:
 \begin{equation}\label{eq:empiricalRisk}
\E[(\hat\Sigma, \hat\Sigma_0^\prime)] {\hat\ell} 
= \E[(\hat\Sigma, \hat\Sigma_0^\prime)]{\loss{\hat\Sigma,\hat\Sigma_0^\prime}}
= \E{\loss{\hat\Sigma,\E{\hat\Sigma_0^\prime}}}
= \E{\loss{\hat\Sigma,\Sigma}} = r
 \end{equation}
Therefore, estimators resulting in consistently lower validation loss can be inferred to produce estimates that are closer to truth than estimators with higher validation loss.
\Kcomment{This is probably not a very insightful question -- but why do we assume that the sample covariance
is what we compare $\hat\Sigma$ when we validate the results.  Our point is that this estimate is not very good.}

Other popular loss functions such as entropy loss do not comply with Eq.~\ref{eq:additivity} and their validation loss is not an unbiased estimate of risk.

Since $\loss{\cdot,\cdot}$ is equivalent to negative normal log likelihood, the above derivation has led to the familiar criterion that the optimal covariance matrix estimator is one that consistently maximizes cross-validated normal log likelihood.

