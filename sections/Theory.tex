\subsection{Problem setup}
Let $x_1,\ldots,x_n$ denote a sample of observations of instantaneous spiking rates of $p$ neurons in time bins $i=1,\ldots,n$.  Each obseravation $x_i$ is a $p\times 1$ vector of spike counts.  We assume that observations independent, identically distributed, i.e.\;all observations are drawn from a common true distribution $x_1,\ldots,x_n \sim f$.  However, this assumptions can be relaxed and corrected for as discussed in this section. We make no assumption about the exact form of distribution $f$. 

Then the true  covariance matrix $\Sigma$ is defined as
\begin{equation}
\Sigma =\mathbb E_z[ (z-\mathbb E_{z\sim f} [z])(z-\mathbb E_z [z])^\T]
\end{equation}
where $\mathbb E_z[g(z)]$ denotes the expected value of $g(z)$ when $z$ is distributed according to $f(z)$. 

The purpose of the covariance estimation procedure is to obtain an estimate $\hat \Sigma$ that is as close as possible to $\Sigma$ given the finite sample  $x_1,\ldots,x_n$. We define a scalar \emph{loss function} $\mathcal L\left(\hat \Sigma, \Sigma\right)$ to measure the distance between $\hat \Sigma$ and $\Sigma$.  The loss function must have minimum at $\hat \Sigma=\Sigma$. 

In particular, we consider the sum-of-squared-error loss
\begin{equation}
\mathcal L_e(\hat \Sigma, \Sigma) =  \| \hat \Sigma - \Sigma\|^2_F 
\end{equation}
where $\| A \|_F = \sqrt{\Tr(AA^\T)}$ denotes the Frobenius matrix norm; $\Tr(\cdot)$ denotes the matrix trace.

We also consider the normal-theory-log-likelihood loss
\begin{equation}
\mathcal L_g(\hat \Sigma,\Sigma) = \ln |\hat \Sigma|  + \Tr(\hat \Sigma^{-1}\Sigma)
\end{equation}
where $|A|$ denotes the determinant of matrix $A$.

Note that both $\mathcal L_e(\hat\Sigma,\Sigma)$ and $\mathcal L_g(\hat\Sigma,\Sigma)$ attain their minima when $\hat\Sigma = \Sigma$.   $\mathcal L_g$ is derived as the negative log likelihood function (scaled and shifted) when $f$ is a multivariate normal distribution.  However, using either of these loss functions does not amount to assuming a specific form of $f$. Other loss functions have been defended in statistical literature \citep{James:1961,Fan:2008}.  



The loss function can be estimated based on another estimate $\hat\Sigma_Y$ of the covariance matrix obtained from an independent \emph{validation} sample $Y = (y_1,\ldots,y_m)\sim f^m$.  

Both $\mathcal L_e(\hat\Sigma,\Sigma)$ and $\mathcal L_g(\hat\Sigma,\Sigma)$ are linear with respect to $\Sigma$ in the sense that
\begin{equation}
\mathcal L\left(\hat \Sigma,\alpha S_1 + (1-\alpha)S_2\right) 
\equiv 
\alpha\mathcal L(\hat\Sigma,S_1) + (1-\alpha)\mathcal L(\hat\Sigma,S_2)
\end{equation}
If we ensure that  $\hat\Sigma_Y$ is unbiased, i.e.\;$\mathbb E_Y\left[\hat \Sigma_Y\right]=\Sigma$, then this linearity implies that 
\begin{equation}
\mathbb E_Y \left[\mathcal L(\hat\Sigma,\hat\Sigma_Y)\right]
=
\mathcal L\left(\hat\Sigma,\mathbb E_Y \left[\hat\Sigma_Y\right]\right)
= 
\mathcal L(\hat\Sigma,\Sigma)
\end{equation}

In other words, the empirical loss $\mathcal L(\hat\Sigma,\hat\Sigma_Y)$ is an ubiased estimator of true loss $\mathcal L(\hat\Sigma,\Sigma)$ provided that $\hat\Sigma_Y$ is unbiased and $\hat\Sigma$ and $\hat\Sigma_Y$ are estimated from independent samples.  Note that $\hat\Sigma$ does not need to be unbiased.  

In practice, recording a separate independent sample may not be sensible.  Instead, the recorded data a split into a training sample and validation sample.  One popular approach is $K$-fold validation, in which the data are split into $K$ roughly equal-sized subsets.  Each subset then successively serves as the validation  sample while the rest of the data make up the training sample. This procedure yields $k$ estimates of the loss, although not quite independent. 
 
\subsection*{Covariance estimators}
It has long been recognized that the empirical covariance 
\begin{equation}
\hat S = \frac 1 n \sum\limits_{i=1}^n x_i x_i^\T
\end{equation}
is statistically inadmissible, meaning that larger sample sizes are required to reduce the loss function to the same level as other, more statistically efficient covariance estimators.

Covariance estimators can be made more efficient by two broad approaches, which are not mutually exclusive: (1) deliberately bias the estimator in order to reduce its variability and (b) reduce the dimensionality of the estimator by choosing a specific parametric model. 

\subsubsection*{Shrinkage estimators}
The most familiar example of the first approach is the linear shrinkage estimator, calculated is a linear mixture of the empirical covariance estimate $\hat S$ and another low-variance target estimator $T$:
\begin{equation}
S_{shrink} = (1-\lambda) \hat S + \lambda T
\end{equation}
A popular target covariance is the identity matrix scaled by the mean sample variance  $T = \frac 1 p \Tr(\hat S) I$.
The optimal value of shrinkage estimate with respect to $\mathcal L_e$ can be estimated analytically from the data \citep{Ledoit:2004,Schafer:2005}.  

\subsubsection*{Dimensionality reduction}
The second broad approach is to impose a structure on the covariance estimate by reducing the number of free parameters in one of its parameterizations. 