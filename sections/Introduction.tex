Pearson correlations between the spiking activity of pairs of neurons, or simply \emph{neural correlations}, are among the most familiar descriptive statistics of neural population activity \citep{Cohen:2011}.  For example, \emph{noise correlations}, \emph{i.e.}~the correlations of stimulus response variability between pairs of neurons, have been of particular interest.  Noise correlations can have profound theoretical implications for stimulus coding \citep{Zohary:1994,Abbott:1999,Averbeck:2006,Berens:2011}, and have been interpreted to indicate detailed and specific functional organization. Such interpretation is supported by a series of discoveries of nontrivial relationships between neural correlations and other aspects of circuit organization such as the physical distance separating the neurons, their synaptic connectivity and stimulus tuning similarity, cortical layer specificity, cell-type specificity, progressive changes in development and in learning, changes due to sensory stimulation and global brain states, and others \citep{Kohn:2005,Smith:2008,Kohn:2009,Goard:2009,Golshani:2009,Renart:2010,Ecker:2010,Ko:2011,Smith:2013,Denman:2013}. 

Neural correlations do not come with ready or unambiguous mechanistic interpretations.  Theoretical and simulation studies show that neural correlations at various temporal scales may arise from any combination of underlying mechanisms.  These include direct synaptic interactions, common or correlated inputs, shared sensory noise, chains of multiple synaptic connections, oscillations, top-down modulation, and background network activity \citep{Perkel:1967b,Shadlen:1998,Salinas:2001,Ostojic:2009,Rosenbaum:2011}.

Early studies of neural correlations were based on measurements from isolated pairs of neurons and their impact on coding was extrapolated to entire populations \citep{Shadlen:1998,Zohary:1994}.  Multineuronal recordings allow estimation of covariance matrices of large populations of neurons.  Such estimates provide more information than the equivalent number of pairwise correlations assessed in isolation. Indeed, the correlation matrix is greater than the sum of its parts: it can be transformed into other representations that accentuate different aspects of the correlation structure and may suggest different mechanistic interpretations. For example, the eigenvalue decomposition of the covariance matrix expresses shared correlated activity components across the population; common fluctuations of population activity may be accurately represented by just a few principal components but will affect all correlation coefficients. In contrast, the off-diagonal elements of the inverse of the correlation matrix constitute scaled partial correlations between neuron pairs, which reflect their specific linear dependencies, after accounting for the activity of all the other recorded cells; a strong interaction between a pair of neurons may be expressed by a single partial correlation but its effects may propagate to multiple correlations and eigenvalues.   The inverse of the covariance matrix plays an important role in decoding schemes such as linear discriminant analysis, for example.  The mutliple  representations of the covariance matrix with their alternative interpretations add both complications and opportunities into the search for fundamental regularities in neural population activity. 

As the number of recorded cells increases, the usual estimations of parameters of the covariance matrix become increasingly noisy and biased. Numerical instabilities arise because, as the amount of recorded data increases only linearly, the number of free coefficients to be estimated in any parameterization of the covariance matrix increases quadratically. 
For example, the large eigenvalues from the sample covariance matrix are biased upward while small eigenvalues are biased downward \citep{Hayes:1981}. Similarly, the coefficients of the inverse covariance matrix require larger sample sizes to be estimated accurately from high-dimensional data than low-dimensional data.

In this study, we pursue two goals: 
\begin{enumerate}[A.\;\;]
	\item Devise good estimators of neural covariance matrices in recordings from large populations of neurons.
	\item Aid interpretation of population activity  by isolating the most essential features or structure of the neural covariance structure.
\end{enumerate}

These two goals are inextricably connected and are best addressed together. Interpretation of neural correlations can be aided by approximating the measured covariance matrix by a simplified, reduced form.  For example, \cite{Malmersjo:2013} analyze the spatial distribution and the graph-theoretical measures of only the highest, most significant correlations.

Estimation of parameters of the covariance matrix from data is inherently challenging, increasingly so for very high-dimensional data. The efficiency of an estimator relates to the rate of its convergence to the true value of the covariance matrix as the function of sample size. The usual estimator, the sample covariance matrix, has been shown to be substantially less efficient than a number of other estimators.  The efficiency of an estimator can be improved through \emph{regularization}, i.e.\;the deliberate biasing of the estimate toward a more simplified approximate representation \citep{Bickel:2006,Bickel:2008,Ledoit:2004,Schafer:2005}. A common intuitive explanation for the effect of regularization is that of adding some prior knowledge about the likely forms of the covariance matrix. Curiously, though, regularization need not reflect any plausible knowledge of the nature of covariance matrices in general or in the particular domain. Convergence can improve due to a purely statistical effect, sometimes known as \emph{Stein's phenomenon}, \emph{i.e.}~the favorable tradeoff between bias and variability, which arises \emph{anytime} a moderate bias toward a less variable target is mixed into an unbiased estimate.  Therefore, an estimator that is shown to converge faster on specific types of data does not necessarily contain within itself some wisdom of the statistical regularities in the data.  However, an estimator that does happen to accommodate the statistical structure of the data in a specific domain, it will gain additional advantage over an \emph{ad hoc} regularizer for that domain. Thus the optimal selection of the estimator for a specific problem with unknown statistical structure becomes an empirical question. And, if a clear winner emerges from a number of strong candidates, its regularization scheme can server as an effective statistical description of the covariance structure. 

In this study, we compared the performance of several covariance estimators for spatially compact groups of 150--300 neurons in layers 2--4 in mouse primary visual cortex during visual stimulation.   The performance of the estimators was evaluated by computing the mean squared error between the optimized covariance estimate fitted to training data and the non-regularized covariance matrix from a separate test data set.  Low-rank covariance estimates performed significantly better than shrinkage estimators, but estimators with sparse partial correlations were more efficient still. Typically, between 3 and 16\% of neuronal pairs were connected by non-zero partial interactions.  Mixed sparse estimators with sparse partial correlations and low-rank components performed comparably. 

As noted above, the optimal covariance estimator is domain-dependent. These specific findings may not spread to all neural circuits.
Improved accuracy estimation accuracy is not the main aim or the only benefit.  
The selection of the optimal regularization scheme in itself serves as an effective descriptor of the dominant correlation structures. The finding that identifiable subsets of partial correlations were important to describe the partial correlation makes such pairs as prime candidates as direct functional interactions.
