\hl{\tiny why correlations:}
Linear correlations between the spiking activity of pairs of neurons, or simply \emph{neural correlations}, are among the most familiar descriptive statistics of neural population activity \citep{Cohen:2011}.   
For example, \emph{noise correlations}, i.e.~the correlations of stimulus response variability between pairs of neurons in sensory areas, have been of particular interest thanks, in part, to their profound theoretical implications for stimulus coding \citep{Zohary:1994,Abbott:1999,Averbeck:2006,Berens:2011}.  
Lending further credence to their role as indicators of detailed and specific functional connectivity have been a series of discoveries of nontrivial relationships between neural correlations and other aspects of circuit organization such as the physical distance separating the neurons, their synaptic connectivity and stimulus tuning similarity, cortical layer specificity, cell-type specificity, progressive changes in development and in learning, changes due to sensory stimulation and global brain states, and others \citep{Kohn:2005,Smith:2008,Kohn:2009,Goard:2009,Golshani:2009,Renart:2010,Ecker:2010,Smith:2013,Denman:2013}. 

\hl{\tiny ambiguity of interpretation:}
Neural correlations do not come with ready or unambiguous mechanistic interpretations.   
Theoretical and simulation studies show that neural correlations at various temporal scales arise from multiple underlying mechanisms such as direct synaptic interactions,  common or correlated inputs, shared sensory noise, chains of multiple synaptic connections, oscillations, top-down modulation, and background network activity \citep{Perkel:1967b,Shadlen:1998,Salinas:2001,Ostojic:2009}

\hl{\tiny opportunities with multineuronal data:}
The neural correlation matrix from multineuronal recordings is greater than the sum of its parts: it can be transformed into other representations  that accentuate different aspects of population activity.  
For example, the eigenvalue decomposition of the covariance matrix, also known as principal component analysis (PCA), expresses shared activity components across the entire population;  common fluctuations of population activity of the entire circuit may be accurately represented by just a few principal components but may affect all correlation coefficients. 
In contrast, the inverse of the correlation matrix reveals the partial correlations between neuron pairs, which reflect their specific linear depedence, conditioned on the activity of all the other recorded cells;
a strong interaction between a pair of neurons may be expressly represented by a single partial correlation but its effect would propagate to multiple correlations and principal components.


\hl{\tiny motivate dimensionality reduction:}
 Covariance estimates from real high-dimensional data  are both noisy and redundant.  The amount of recorded data increases only linearly with population size whereas the number of free parameters in the covariance matrix increases quadratically; as a result, empirical covariance estimates for multidimensional data are  accumulate substantial redundancy and noise. 

To reduce the estimation error, statistically efficient estimators are regularized.  One regularization approach is to shrink the estimate toward a less variable low-dimensional estimate \citep{Schafer:2005}.  Another regularization approach, often called \emph{sparsification}, reduces the dimensionality of the estimate by setting its least significant parameters to zero. 
Regularized estimates perform favorably on new samples of data compared to the full model, provided the amount of regularization is chosen carefully.

\hl{\tiny choosing between regularization schemes}
Estimator improvements due to regularization depend profoundly on the parameterization to which it is applied.  Sparsification of the eigenspectrum produces a low-rank estimate of the covariance matrix also known as truncated PCA \citep{Rothman:2008} or, allowing for additional independent variances, as factor analysis (FA) \citep{Fan:2008}.  Sparsification of the inverse covariance is known as covariance selection \citep{Dempster:1972,Friedman:2008} and is related to finding a graphical model in which a large fraction of neuron pairs are conditionally independent of each other.  Representations that are capable of concentrating the significant real effects in a small number of parameters will reap greatest benefits from sparsification over ones that distribute the real effects widelyacross many parameters.  Random noise tends to be broadly distributed across all parameters in any parameterization. 


\hl{\tiny our approach and summar of findings} 
In this study, we compared the performance of several covariance estimation schemes for spatially compact groups of 150--300 neurons in layers 2-4 in mouse primary visual cortex during visual stimulation.   The performance of the estimators was evaluted by computing the mean squared error between the optimized covariance estimate fitted to training data and the non-regularized covariance matrix from a separate test data set.  Low-rank covariance estimates performed significantly better than shrinkage estimators, but estimators with sparse partial correlations were more efficient still. Typically, between 3 and 16\% of neuronal paris were connected by non-zero partial interactions.  Mixed sparse estimators with sparse partial correlations and low-rank components performed comparably. 

As noted above, the optimal covariance estimator is domain-dependent. 
These specific findings may not spread to all neural circuits.
Improved accuracy estimation accuracy is not the main aim or the only benefit.  
The selection of the optimal regularization scheme in itself serves as an effective descriptor of the dominant correlation structures. The finding that a identifiable subsets of partial correlations were important to describe the partial correlation makes such pairs as prime candidates as direct functional interactions. 