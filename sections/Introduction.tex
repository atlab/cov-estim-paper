\hl{\tiny why correlations:} Linear correlations between the spiking activity of pairs of neurons, or simply \emph{neural correlations}, are among the most familiar descriptive statistics of neural population activity \citep{Cohen:2011}.  For example, \emph{noise correlations}, \emph{i.e.}~the correlations of stimulus response variability between pairs of neurons, have been of particular interest.  Noise correlations can have profound theoretical implications for stimulus coding \citep{Zohary:1994,Abbott:1999,Averbeck:2006,Berens:2011}, and have been interpreted to indicate detailed and specific functional organization. Such interpretation is supported by a series of discoveries of nontrivial relationships between neural correlations and other aspects of circuit organization such as the physical distance separating the neurons, their synaptic connectivity and stimulus tuning similarity, cortical layer specificity, cell-type specificity, progressive changes in development and in learning, changes due to sensory stimulation and global brain states, and others \citep{Kohn:2005,Smith:2008,Kohn:2009,Goard:2009,Golshani:2009,Renart:2010,Ecker:2010,Smith:2013,Denman:2013}. 

\hl{\tiny ambiguity of interpretation:} Neural correlations do not come with ready or unambiguous mechanistic interpretations.  Theoretical and simulation studies show that neural correlations at various temporal scales may arise from any combination of underlying mechanisms.  These include direct synaptic interactions, common or correlated inputs, shared sensory noise, chains of multiple synaptic connections, oscillations, top-down modulation, and background network activity \citep{Perkel:1967b,Shadlen:1998,Salinas:2001,Ostojic:2009,Rosenbaum:2011}

\hl{\tiny opportunities with multineuronal data:} Early studies of neural correlations were based on measurements from isolated pairs of neurons and their impact on coding was extrapolated to entire populations \citep{Shadlen:1998,Zohary:1994}.  Multineuronal recordings allow estimation of covariance matrices of larger populations.  Such estimates provide more information than the equivalent number of pairwise correlations assessed in isolation. Indeed, the correlation matrix is greater than the sum of its parts: it can be transformed into other representations that accentuate different aspects of the correlation structure and may suggest different mechanistic interpretations. For example, the eigenvalue decomposition of the covariance matrix expresses shared correlated activity components across the population; common fluctuations of population activity may be accurately represented by just a few principal components but will affect all correlation coefficients. In contrast, the off-diagonal elements of the inverse of the correlation matrix constitute scaled partial correlations between neuron pairs, which reflect their specific linear dependencies, after accounting for the activity of all the other recorded cells; a strong interaction between a pair of neurons may be expressed by a single partial correlation but its effects may propagate to multiple correlations and eigenvalues.

\hl{\tiny motivate regularization} Estimation of parameters of the covariance matrix from data is inherently challenging, increasingly so for very high-dimensional data. As the amount of data grows only linearly with population size, the number of free parameters in a complete representation of the covariance matrix increases quadratically, multiplying opportunities for spurious patterns to emerge among the correlations.  Although the estimation error of each covariance coefficient, in isolation, is unaffected by the dimensionality of the data, the estimation error of other parameters of the covariance matrix increases with population size.  
For example, the large eigenvalues of in the sample covariance matrix are biased upward while small eigenvalues are biased downward, with both variance and bias increasingly large with increasing data dimensionality \citep{Hayes:1981}.  Similarly, the coefficients of the inverse covariance matrix require larger sample sizes to be estimated accurately in high-dimensional data.

\hl{\tiny forms of regularization} The convergence of the estimate of multivariate parameters to their true values can be improved through \emph{regularization},  Regularization is the deliberate biasing or \emph{shrinkage} of the empirical estimate toward a less variable \emph{target estimate} \citep{Schafer:2005} or the deliberate restriction of the estimate to a lower-dimensional space \citep{Dempster:1972,Fan:2008,Friedman:2008,Rothman:2008}.  A common intuitive explanation for the effect of regularization is that regularization adds some prior knowledge about the likely forms of the covariance matrix. Curiously, though, regularization need not reflect any plausible knowledge of the nature of covariance matrices in general or in the particular domain. Convergence can improve due to a purely statistical effect, sometimes known as \emph{Stein's phenomenon}, \emph{i.e.}~the favorable tradeoff between bias and variability, which arises \emph{anytime} a moderate bias toward a less variable target is mixed into an unbiased estimate.  Therefore, an estimator that is shown to converge faster on specific types of data does not necessarily contain within itself some wisdom of the statistical regularities in the data.  However, an estimator that does happen to accommodate the statistical structure of the data in a specific domain, it will gain additional advantage over an \emph{ad hoc} regularizer for that domain. Thus the optimal selection of the estimator for a specific problem with unknown statistical structure becomes an empirical question. And, if a clear winner emerges from a number of strong candidates, its regularization scheme can server as an effective statistical description of the covariance structure. 

\hl{\tiny approach and summary of findings} In this study, we compared the performance of several covariance estimators for spatially compact groups of 150--300 neurons in layers 2--4 in mouse primary visual cortex during visual stimulation.   The performance of the estimators was evaluated by computing the mean squared error between the optimized covariance estimate fitted to training data and the non-regularized covariance matrix from a separate test data set.  Low-rank covariance estimates performed significantly better than shrinkage estimators, but estimators with sparse partial correlations were more efficient still. Typically, between 3 and 16\% of neuronal pairs were connected by non-zero partial interactions.  Mixed sparse estimators with sparse partial correlations and low-rank components performed comparably. 

\hl{\tiny summary of conlusions} As noted above, the optimal covariance estimator is domain-dependent. These specific findings may not spread to all neural circuits.
Improved accuracy estimation accuracy is not the main aim or the only benefit.  
The selection of the optimal regularization scheme in itself serves as an effective descriptor of the dominant correlation structures. The finding that identifiable subsets of partial correlations were important to describe the partial correlation makes such pairs as prime candidates as direct functional interactions.